{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.14: Introduction To Pytorch\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/10G4hNPwtIq0urT--yN3VFznhqaS8kalBrlLXfcd9bvE/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to train a neural network classifier in pytorch both on local CPU and cloud GPU.\n",
    "\n",
    "**Learning goals:**\n",
    "- understand the difference between `ndarray` and `Tensor`\n",
    "- carry out basic operations on `Tensor`s\n",
    "- backpropagate through a computational graph with `autograd`\n",
    "- build and train a neural network banknote classifier\n",
    "- setup a google colab notebook\n",
    "- train a neural network on a GPU\n",
    "- compare the pros and cons of pytorch and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 0. Setup\n",
    "\n",
    "To use pytorch, we have to install the `torch` package. It was added to this projects's `Pipfile`, so please run:\n",
    "\n",
    "    pipenv install\n",
    "    \n",
    "in the repo's root directory to install the latest dependencies.\n",
    "\n",
    "\n",
    "Remember your first `import numpy as np`? Time has flown by! This is the start of another exciting chapter, your first pytorch import ðŸŽŠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "\n",
    "### 1.1 Tensor Creation\n",
    "\n",
    "Welcome to the world of pytorch! ðŸ”¥\n",
    "\n",
    "The main class here is the `Tensor`. It's a scary mathematical name, but according to [pytorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) themselves:\n",
    "\n",
    "> Tensors are similar to NumPyâ€™s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "This means that we can expect their interface to be similar to `ndarray`. ðŸ˜Œ\n",
    "\n",
    "The preferred way to build `Tensor`s, is with the `torch.tensor()` constructor. This is similar to `np.array()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([42, 666])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch also offers a bunch of other useful constructors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0, 1337, 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic Operations\n",
    "\n",
    "Just like NumPy, pytorch integrates closely with python by overloading many common operators. We can use python list slicing ðŸ”ª:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0, 1, 1, 2, 3, 5, 8, 13])\n",
    "print(a[3])\n",
    "print(a[3:6])\n",
    "print(a[1:7:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use arithmetic operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(a + b)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piecewise operations are the default in pytorch, just like for `ndarray`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â 1.3 Tensor Data Types\n",
    "\n",
    "Remember that NumPy is super fast because it uses its own data types. Maybe pytorch uses the same trick! Let's check the type of our tensors elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "type(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ˜‘ Ok we already knew this was a `Tensor`, but what data type does it contain? We can check through its `.dtype` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to convert this torch value to a python scalar, we can use the `.item()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to convert any `Tensor` to a numpy array, we can use the `.numpy()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a _bridge_ meaning that `a` and `b` share their underlying memory location. Careful, as changing one will change the other! \n",
    "\n",
    "Finally we can convert `ndarray`s to `Tensor`s with the `torch.from_numpy()` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 2. Autograd\n",
    "\n",
    "### 2.1 Backpropagation\n",
    "\n",
    "Pretty familiar grounds so far! But we haven't really shown what makes `Tensor`s special. ðŸŒˆ The official documentation mentions GPU acceleration, but we'll keep that for the end of the lecture. Until then, let's focus on another pytorch killer feature: the `grad_fn`.\n",
    "\n",
    "By providing the `requires_grad=True` argument to our `Tensor` constructors, we tell pytorch to track all operations on it. ðŸ“¡ These operations are stored in their `.grad_fn` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y is a `Tensor` \"born\" from the addition $x + 2$ on the `Tensor` `x` which had `requires_grad=True`. Therefore pytorch keep track of this operation, and saved it under `.grad_fn`.\n",
    "\n",
    "This `requires_grad` property is passed on to the children `Tensors`, meaning we can create a _chain_ of `.grad_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y**2\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch therefore keeps track of the _computational graph_ created from root tensors with `requires_grad=True`.\n",
    "\n",
    "But why are going through this hassle? Well tensors secretly work with `torch.autograd`, the [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) package ðŸ•µï¸â€â™€ï¸. So by saving the operations linking `Tensors`, `autograd` is then able to numerically evaluate the derivatives of all nodes in the computational graph. \n",
    "\n",
    "When we call `.backward()` on a `Tensor`, we ask `autograd` to use the chain rule and evaluate the gradients through the nodes of the computational graph. The gradients are then saved on each `Tensor` under `.grad`. These are the partial derivatives of the operation on which we called `.backward()` with respect to that `Tensor`. \n",
    "\n",
    "This all sounds familiar.... Propagating gradients through a computation graph with the chain rule? Caching intermediate derivative values? `.backward()` is therefore an implementation of _backpropagation_. Of course, pytorch's end goal here is to use these gradients as part of _gradient descent_ to optimize neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To understand exactly what gradient descent with pytorch looks like, let's implement a simple two layer linear neural network, less glamourously known as a âœ¨linear regression modelâœ¨ (checkout lecture 3.5 for a refresher!) We'll use two features and a single _example_ to update the model parameters.\n",
    "\n",
    "First let's initialize our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = torch.tensor(-2.0, requires_grad=True)\n",
    "theta1 = torch.tensor(3.0, requires_grad=True)\n",
    "theta2 = torch.tensor(-1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define our only training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.3, -0.1])\n",
    "y = torch.tensor(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that the linear regression hypothesis is:\n",
    "\n",
    "$$ y = \\sum_{i=0}^{n}\\theta_{i}x_{i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = theta0 + x[0] * theta1 + x[1] * theta2\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our initial prediction is $-1$ ... pretty far from the label $0.4$! Let's calculate the associated MSE loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y_predict - y)**2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss is the final node in our computation graph. By calling `.backward()` on it, we tell `autograd` to backpropagate through the edges, and calculate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... that was it? Backpropagation felt much more complicated in the lecture 3.12 slides ðŸ˜…Let's check the gradient $\\frac{dJ}{d\\theta_{0}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty cool, but this value of $-2.8$ doesn't get us anywhere on its own. We need to use it as part of a gradient descent update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"value of theta0 before gradient descent update: {theta0}\")\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "theta0 = theta0 - alpha * theta0.grad\n",
    "theta1 = theta1 - alpha * theta1.grad\n",
    "theta2 = theta2 - alpha * theta2.grad\n",
    "\n",
    "print(f\"value of theta0 after gradient descent update: {theta0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\theta_{0}$ was updated by taking a step in the opposite direction to the gradient. Let's check how this affected our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = theta0 + x[0] * theta1 + x[1] * theta2\n",
    "new_loss = (y_predict - y)**2\n",
    "print(f\"Loss before gradient descent update: {loss}\")\n",
    "print(f\"Loss after gradient descent update: {new_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss was minimized, and if we repeated this process many times with with many example, we might obtain $\\theta$s which correctly predict the labels `y`.\n",
    "\n",
    "Note that almost _all_ of the code above is mathematical operations. Pytorch is appreciated by data scientists because it abstracts away the machine learning heavy lifting (e.g backpropagation), but still lets them control the low level operations. \n",
    "\n",
    "Sometimes we don't want to write all the low level maths, and would rather get on training our neural networks. ðŸ˜… Let's use pytorch to train the exact same fake banknote detection classifier as last lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our dataset in a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank_note.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are scaled and ready to go! ðŸ‹ï¸â€â™€ï¸We'll use all 4 features and put them in a feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "y = df['is_fake'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the linear regression example above, we don't have access to a `.fit(X, y)` method which takes care of feeding the dataset to the neural network. Therefore, we are going to want to manually create _examples_ , which are pairs of (features, label), i.e pairs of rows from `X` and `y`. We can create these tuple pairs with the handy [`.zip()`](https://realpython.com/python-zip-function/) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(X, y))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first example here has:\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}1.12\\\\1.15\\\\-0.98\\\\0.35\\end{bmatrix}; y = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Neural Network Structure\n",
    "\n",
    "Our dataset is ready, so now we want to train a neural network! We saw in the previous section that pytorch allows to backpropagate through any mathematical operation. However, passing around raw `x + y ` functions and `Tensors` can lead to a lot of copy-pasting and isn't very bug-safe. ðŸ˜¬ Instead we'd like to abstract away the computational graph in a dedicated class. Pytorch makes this easy with the `nn.Module`.\n",
    "\n",
    "We extend the `nn.Module` abstract class, and create a `Net` class. All we have to do is:\n",
    "- define our _layers_ in the `__init__` constructor\n",
    "- define the computational graph in the `.forward()` method. For neural networks, this often includes appplying activation functions, and feeding the output of one layer into the next. `.forward()` is simply the chained function defined by the neural network, which takes features as input and returns predictions as output.\n",
    "\n",
    "Since we are lazy, we'll use the ready-made pytorch `nn.Linear` layer. Just like keras, this will infer the required number of model parameters from its inputs and outputs. `nn.Linear` also takes care of setting `requires_grad` on all the right `Tensor`s, so we know backpropagation will work.\n",
    "\n",
    "Let's put all of this together in a 2 hidden layer neural network with ReLU activations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # create the layers\n",
    "        self.dense1 = nn.Linear(4, 6)\n",
    "        self.dense2 = nn.Linear(6, 6)\n",
    "        self.dense3 = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first hidden layer\n",
    "        x = F.relu(self.dense1(x))\n",
    "        # second hidden layer\n",
    "        x = F.relu(self.dense2(x))\n",
    "        # output layer\n",
    "        x = torch.sigmoid(self.dense3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a neural network in pytorch ðŸŽŠ\n",
    "\n",
    "ðŸ§  What do the arguments to `nn.Linear()` represent? Why are we using these particular values here?\n",
    "\n",
    "Let's test its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = torch.tensor([0.1, -0.2, 0.3, 0.4])\n",
    "net(x_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, `nn.Module` does some background python magic for us so we can call your network directly as a function. ðŸ˜®\n",
    "That prediction doesn't look great though, because we haven't trained the model yet!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dataset Iteration\n",
    "For this, we want to loop through our `dataset`. But we also don't really want to implement batches or shuffling of examples... Luckily, pytorch takes care of that for us too. \n",
    "\n",
    "We use a `DataLoader` to iterate through our dataset. The `torch.utils.data` can load many dataset types, including our list of tuples of `ndarrays`. Checkout the [documentation](https://pytorch.org/docs/stable/data.html) for more details! All we have to do, is pass it in the constructor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ds_loader` is a python _iterable_ , meaning use it in `for` loops, or peek into the first batch of examples like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds_loader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of values! The shape of the features should always be `(batch_size, dims)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds_loader)[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Loss and Optimization\n",
    "\n",
    "There are two missing pieces to our training. Just like the `nn.Linear` layers, we don't really want to implement the _loss_ and the _optimizer_ for our neural network. Recall that the loss is the mathematical function defining the average model error for given predictions and labels, $J$. The optimizer defines the update of each model parameter for a given gradient with respect to that $\\theta$. These can get complicated! Instead, we'll use the `torch.nn` and the `torch.optim` modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `criterion` is a simple function which calculates the loss:\n",
    "```\n",
    "loss = criterion(predictions, labels)\n",
    "``` \n",
    "We choose binary cross-entropy loss (`BCELoss`) since we are dealing with a binary classification problem.\n",
    "\n",
    "- `optimizer` will fetch the `.grad` fields directly on the `net.parameters()`, and adapts the learning rate according to its algorithm. The update is done in place by calling the `.step()` method:\n",
    "```\n",
    "optimizer.step()\n",
    "```\n",
    "We pick `Adam` because adam rocks ðŸ¤˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Optimization\n",
    "\n",
    "We're now ready to optimize! Let's start easy, just like our pytorch linear regression. We'll carry out _one_ gradient descent update, with only the first batch of data.\n",
    "\n",
    "First, we load the batch from our `DataLoader`. We have to convert its `dtype` and reshape the labels, because like `sklearn`, pytorch likes its input a certain way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the first batch\n",
    "inputs, labels = list(ds_loader)[0]\n",
    "# pytorch likes floats\n",
    "inputs = inputs.float()\n",
    "# view = np.reshape(), need a matrix here\n",
    "labels = labels.float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can predict the outputs using our neural network and calculate the associated cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "outputs = net(inputs)\n",
    "# loss from predictions & labels\n",
    "loss = criterion(outputs, labels)\n",
    "print(f\"Loss before gradient descent update: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use gradient descent to update the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always zero the parameter gradients before calling .backward()\n",
    "optimizer.zero_grad()\n",
    "# backpropagation\n",
    "loss.backward()\n",
    "# gradient descent update\n",
    "optimizer.step()\n",
    "\n",
    "# calculate the new loss\n",
    "outputs = net(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "print(f\"Loss after gradient descent update: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Take your time to understand the above code, and step through the different stages. What happens in the `.net()` call? what about the `.step()` method? \n",
    "\n",
    "We reduced the loss! This is great progress, but we'd like to find the global minimum, not just go \"down\" one step... For this, we have to repeat the updates for all batches and several epochs. Let's go! ðŸ¤ \n",
    "\n",
    "Pytorch uses a dynamic computation graph, so it starts from scratch everytime we make a forward pass through the neural network: `net(inputs)`. This means we can just loop the gradient descent step without worrying about global state or breaking things. We'll also set the pytorch and NumPy random seeds to improve [reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(666)\n",
    "\n",
    "# initialization\n",
    "net = Net()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "losses = []\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(100):\n",
    "    print(f'epoch {epoch} ')\n",
    "    \n",
    "    # loop over batches\n",
    "    for i, data in enumerate(ds_loader):\n",
    "        # data loading\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float().view(-1, 1)\n",
    "        \n",
    "        # prediction\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print('finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that was fast! Pytorch's custom data types and `autograd` package make for swift gradient calculations. And we haven't even used GPUs yet ðŸ˜\n",
    "\n",
    "ðŸ§  How many epochs did we train our neural network for?\n",
    "\n",
    "We stored the mean loss of each batch in the `losses` list. Let's visualize our loss curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "fig = plt.figure(dpi=100)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(losses, lw=1)\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('steps')\n",
    "ax.set_title('Loss Curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve looks just the same as last lecture with keras, which means we have successfully trained our pytorch neural network. ðŸŽŠ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction\n",
    "\n",
    "Just like we did with our untrained model, we can call the `Net` directly as a function to predict the label a given input `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banknote = [0.0, -0.1, 0.3, 0.2]\n",
    "x_predict = torch.tensor(banknote)\n",
    "net(x_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network is very confident that this banknote is genuine ðŸ’¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "### 6.1 BCEWithLogits\n",
    "\n",
    "For binary classification, pytorch recommends the use of [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). According to the documentation:\n",
    "\n",
    "> This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n",
    "\n",
    "Sounds like a good idea for our fake banknote detector. ðŸ‘Œ\n",
    "\n",
    "ðŸ’ª Implement and train the same pytorch neural network model for banknote classification, but this time, use `BCEWithLogitsLoss`.\n",
    "- since `BCEWithLogitsLoss` already incorporates the output layer sigmoid activation, you'll have to rewrite the `Net` class and remove it\n",
    "- use the same hyperparameters as above: `batch_size=32`, `epochs=100`, `Adam` optimizer, ...\n",
    "- store your losses per batch in `losses`, the loss curve will be automatically plotted when running the unit test\n",
    "- the loss curve should look exactly the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bce_with_logits():\n",
    "    assert losses, \"Can't find losses. Did you use the correct variable name?\"    \n",
    "    assert np.array(losses[-10:]).mean() < 0.005, \"It doesn't look like your loss converged\"\n",
    "    print('Success! ðŸŽ‰')\n",
    "    \n",
    "fig = plt.figure(dpi=100)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(losses, lw=1)\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('steps')\n",
    "ax.set_title('Loss Curve')\n",
    "\n",
    "test_bce_with_logits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Batch size analysis\n",
    "\n",
    "ðŸ’ªðŸ’ª Analyse the effect of batch size on neural network optimization, by plotting the loss curves of models with different batch sizes.\n",
    "- look at lecture 3.13 for an example... but implement this with pytorch!\n",
    "- plot the loss curves side by side, or the graphs will be unreadable\n",
    "- wrap the neural training in a function to easily iterate through different batchsizes\n",
    "- either reduce the number of epochs to ~ 20, or find a way to set individual epochs for each batch size. Otherwise, the small batch sizes will take a long time to train\n",
    "- the graph is the unit test ðŸ™ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, losses in losses_dict.items():\n",
    "    fig = plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses, lw=1)\n",
    "    ax.set_xlabel('batch')\n",
    "    ax.set_ylabel('steps')\n",
    "    ax.set_title(f'batch_size={batch_size}');\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Think about your results. Do they agree with the results from lecture 3.13?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GPUs\n",
    "\n",
    "We've seen how pytorch adds backpropagation support to Tensors, and provides some useful methods to iterate through datasets, calculate common losses, and perform gradient updates on model parameters.\n",
    "\n",
    "Tensors have another trick up their sleeve: they can be used on a _GPU_. As mentioned in the slides, GPUs can parallelize matrix operations, and have a larger memory bandwidth. Training a neural network on a GPU can be orders of magnitude faster than a CPU.\n",
    "\n",
    "Let's move to google colab and play with some fancy hardware! ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pytorch vs Keras vs Tensorflow\n",
    "\n",
    "We have learned about _two_ deep learning frameworks, keras, and pytorch. By now, it should be obvious that these libraries take a different approach to neural networks. Keras' api is more abstracted and OOP focused, whilst pytorch augments mathematical functions. There is no \"better\" choice, as each will shine for different usecases:\n",
    "\n",
    "**keras**\n",
    "- concise code\n",
    "- little theoretical understanding required\n",
    "- good for simple projects and fast POCs\n",
    "\n",
    "**pytorch**\n",
    "- lower level control\n",
    "- more mathematical\n",
    "- good for growing projects and advanced NNs\n",
    "- popular nowadays ðŸ˜Ž great community support\n",
    "\n",
    "Tensorflow, which we have been running as keras backend, is another popular open source deep learning library. It is on par with pytorch in terms of customization and low level control, and sells itself as being production and deployment focused. However, more and more machine learning engineers are swithing to pytorch, which is considered easier to develop with. This might change in the near future, so it's good to keep an eye on the evolution of the ML open source ecosystem ðŸ‘€.\n",
    "\n",
    "All in all, coders are allowed to have personal preferences ðŸ’â€â™‚ï¸, so you should pick the tools that best solve your problem _and_ that you are most comfortable with. Now you have two to choose from âœŒï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Today, we discovered a new deep learning library, **pytorch**. We first explained how **GPUs** can accelerate neural networks with parallel matrix operations, and large memory bandwitdth. Computing time is a bottleneck in neural network optimization, so **faster** training leads to **more powerful** models. We listed tensorflow & pytorch as the most popular DL frameworks which can interface with GPUs. We then learned how to use pytorch: first by creating **`Tensor`s**, then by backpropagating through a computation graph with the **`autograd`** package. We applied this to neural networks by extending the **`nn.Module`**, where we defined **layers** and the **`.forward()`** method. We **looped** this code by iterating through a **`DataLoader`** , and recreated the banknote authentication classifier from last lecture. Finally, we ported the pytorch neural network to **Google colab** , and trained it on a **GPU runtime environment**.\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "\n",
    "- [**Slides**](https://docs.google.com/presentation/d/10G4hNPwtIq0urT--yN3VFznhqaS8kalBrlLXfcd9bvE/edit?usp=sharing)\n",
    "- [Pytorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)  \n",
    "Fast official pytorch tutorial, great for a refresher on basics\n",
    "- [Collection of DL models implemented in tf and pytorch](https://github.com/rasbt/deeplearning-models)   \n",
    "Great to have bookmarked for inspiration / debugging when creating neural networks with pytorch or tensorflow\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Pytorch ecosystem](https://pytorch.org/ecosystem/)  \n",
    "List of the many libraries extending pytorch\n",
    "- [ignite](https://github.com/pytorch/ignite)  \n",
    "Pytorch wrapper similar to the keras api\n",
    "- [Pytorch for fast.ai](https://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/)  \n",
    "Blogpost explaining fast.ai's move from keras+tensorflow to pytorch\n",
    "- [Pytorch reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)  \n",
    "Notes about randomness in pytorch\n",
    "- [Why are GPUs well suited to deep learning](https://www.quora.com/Why-are-GPUs-well-suited-to-deep-learning)  \n",
    "Quora thread with a detailed explanation of GPUs advantages over CPUs\n",
    "- [Carbon footprint of large language models](https://arxiv.org/pdf/1906.02243.pdf)  \n",
    "Famous paper investigating the environmental impact of ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
