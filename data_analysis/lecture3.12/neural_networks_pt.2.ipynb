{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.12: Neural Networks Pt.2\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/1RLRcvClkzBXLrixydfXnEu6b91lqurEeeH7oq2e9Fuc/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to train a neural network classifier and regressor with keras.\n",
    "\n",
    "**Learning goals:**\n",
    "\n",
    "- train a neural network classifier\n",
    "- visualize training by plotting the loss curve\n",
    "- reproduce training by setting random seeds\n",
    "- analyse the effect of network architecture on training speed\n",
    "- train a neural network regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "This notebook uses the keras and tensorflow deep learning libraries. If you haven't already, please follow the setup steps in the last notebook (3.11) to correctly install these dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Munging\n",
    "\n",
    "Let's find some counterfeiters again! We load the banknote authentication dataset in a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank_note.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are scaled and ready to train hard! ðŸ‹ï¸â€â™€ï¸We'll use all 4 features since we already went through decision boundary visualization last lecture. This time, our focus is on the _optimization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "y = df['is_fake'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Before we can run gradient descent and backpropagation, we need to create the structure of our neural network. Just like last lecture, this can be done with keras' `Sequential` api.\n",
    "\n",
    "We'll try a neural network with 2 `Dense` hidden layers of 6 neurons each. We'll use one of the most common activation functions, `relu`, popular for its optimization speed and regularization properties (more info [here](https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions)).  \n",
    "The input dimension is 4 since we are using 4 features.  \n",
    "The output layer has 1 `sigmoid` neuron since we are solving a _binary_ classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(6, activation='relu', input_dim=4),\n",
    "    Dense(6, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the neural network structure with keras' `.summary()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Do these parameter counts still make sense from last lecture?\n",
    "\n",
    "What is the value of these 79 model weights? We haven't _trained_ our model yet, so the $\\theta$s have not been optimized. However, when setting up the structure of our neural network, keras has _initialized_ its weights. \n",
    "\n",
    "Recall from lecture 3.5 that before gradient descent can iteratively update $\\theta$s, their values must _randomly initialized_. It's like choosing a starting point on the loss function mountain from which to go downhill, towards the minimum of $J$. ðŸ” It might sound strange to pick a _random_ start, but in practice this turns out to be a good idea. We'll see more next lecture about how randomness can help neural network optimization.\n",
    "\n",
    "There are [many ways](https://keras.io/api/layers/initializers/) of randomly initializing weights, but the keras default is pretty good. So let's not worry about this hyperparameter just yet, and have a look at our randomly initialized model parameters.\n",
    "\n",
    "We can use the keras method [`.get_weights()`](https://keras.io/api/models/model_saving_apis/#get_weights-method). It has a strange way of providing the parameters: it returns a list of alternating weights & bias weights for each successive layer. Recall that bias weights are just the $\\theta_{0}$s of each weight vector, acting on the bias nodes.\n",
    "\n",
    "e.g for our neural architecture:\n",
    "```python\n",
    "# layer1 weights\n",
    "weights[0]\n",
    "# layer1 bias weights\n",
    "weights[1]\n",
    "# layer2 weights\n",
    "weights[2]\n",
    "# layer2 bias weights\n",
    "weights[3]\n",
    "# layer3 weights\n",
    "weights[4]\n",
    "# layer2 bias weights\n",
    "weights[5]\n",
    "```\n",
    "\n",
    "Since there are 79 weights total, let's just peek at the weights of the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look pretty random indeed! ðŸ¤ª The first row is all the $\\theta_{1}$s mapping the _first_ feature to our six hidden neurons, the second row is all the $\\theta_{2}$ mapping the _second_ feature to our six hidden neurons, etc\n",
    "\n",
    "Now that we have initialized our neural network, we must _compile_ it. This is just a way of configuring the model for training. For example, we haven't specified our _loss function_ yet, so keras has no idea if we're trying to solve a classification or a regression task here.\n",
    "\n",
    "Compilation is done with the ... [`.compile()`](https://keras.io/api/models/model_training_apis/#compile-method) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using `binary_crossentropy` as loss function. This + the choice of `sigmoid` activation means our last layer will be a simple logistic regression layer.\n",
    "\n",
    "... who is `adam` though? ðŸ¤·â€â™‚ï¸Neural network optimization is tricky, and there are many flavours of gradient descent available. More on this next lecture, where we will get to know `adam` better. ðŸ¤\n",
    "\n",
    "Until then, let's train our model! Now that it is configured for training, keras has all the necessary information to start optimizing the weights. Similarly to the sklearn api, we use the [`.fit()`](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
    "\n",
    "Once again we have to sneak in a couple of extra arguments: \n",
    "- In this example, `epochs` is the number of gradient descent iterations. Keras doesn't automatically decide when the loss converged, so we have to specify a cut-off. The full definition of `epochs` will be provided next lecture.\n",
    "- `batch_size` specifies the number of examples used for each gradient descent step. We haven't talked about why changing this hyperparameter might help with optimization, so more on this next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=2000, batch_size=len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that took a while! âŒ›ï¸ Keras did have a lot of work to do...\n",
    "\n",
    "ðŸ§ ðŸ§  List all the steps that keras went through to `.fit()` this neural network. \n",
    "\n",
    "keras printed out a lot of information. The part we are most interested in is the `loss` value at each step of gradient descent. It looks like it's decreasing throughout the optimization, which is a good sign! We can visualize this directly by plotting the [loss curve](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic).\n",
    "\n",
    "A [History callback](https://keras.io/api/callbacks/) stored a bunch of training information at each training epoch, which we can accesss as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "loss = history.history['loss']\n",
    "\n",
    "fig = plt.figure(dpi=120)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(loss)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_title('Loss Curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss curve is an effective way of checking that everything went smoothly during training (here's some extra [tips](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic) on how to interpret different loss curve profiles). Our loss curve is decreasing, and converges towards the end of our optimization. This means that gradient descent is complete and that the model is fully trained!\n",
    "\n",
    "ðŸ§  What should you do if the loss curve was still strongly decreasing towards your final epochs?\n",
    "\n",
    "We won't visualize the model's decision boundary or analyse its predictions in this notebook, since we've already done this in notebook 3.11. However, we can peek at our updated model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are different from our randomly initialized weights because they were _optimized_ by keras during training.\n",
    "\n",
    "ðŸ§ ðŸ§  Do these model parameters look overfit? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis\n",
    "\n",
    "### 4.1 Training Reproducibility\n",
    "\n",
    "Random weight initialization is one of _many_ random processes used in neural network optimization. This means that our training procedure above was not _reproducible_. We can prove this by training two consecutive models. The `create_neural_network()` and `train_neural_network()` functions will prevent us from copy-pasting code 924 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(layers):\n",
    "    model = Sequential()\n",
    "    first_layer = layers.pop(0)\n",
    "    model.add(Dense(first_layer, activation='relu', input_dim=4))\n",
    "    for layer in layers:\n",
    "        model.add(Dense(layer, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "              \n",
    "def train_neural_network(X, y, layers, **kwargs):\n",
    "    model = create_neural_network(layers)\n",
    "    model.fit(X, y, batch_size=len(X), **kwargs)\n",
    "    return model\n",
    "\n",
    "model1 = train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "model2 = train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "\n",
    "print(model1.get_weights()[0])\n",
    "print(model2.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized weights of these two models aren't the same at all! This doesn't necessarily mean that one of these models is bad, but it is still inconvenient if we ever try to recreate some amazing results. ðŸ˜°\n",
    "\n",
    "We know that NumPy is THE math library in python, so maybe we can try setting NumPy's random seed before each training to harmonize the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "model1 = train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "\n",
    "np.random.seed(1337)\n",
    "model2 = train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "\n",
    "print(model1.get_weights()[0])\n",
    "print(model2.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not enough! This doesn't work because we are using a tensorflow backend, and tensorflow is _special_ ðŸ’â€â™‚ï¸ and uses its own random number generator. We have to reset the _tensorflow_ seed before each training too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(666)\n",
    "model1 = train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(666)\n",
    "model2 = train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "\n",
    "print(model1.get_weights()[0])\n",
    "print(model2.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are close, but not quite the same. The differences are actually due to numerical errors, so our model was reproduced, but our weights are just not that accurate ðŸ’˜. In fact, we'll see that for complex neural networks trained across several GPUs, it is sometimes impossible to train deterministically. This makes it even more important to have solid data engineering, and somewhere to save our model weights. More on this next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture slides, we have described neural networks as _slow_. ðŸŒ This is in part due to their large number of parameters. There are many ways of layering these weights into a neural network however, so let's investigate how neural architecture affects training speed.\n",
    "\n",
    "We'll use our `train_neural_network()` function, and the [`timeit`](https://stackoverflow.com/questions/29280470/what-is-timeit-in-python) magic function will help us measure the performance of the models. We are comparing three neural networks:\n",
    "- 2 hidden layers of 6 neurons each\n",
    "- 2 hidden layers of 100 neurons each\n",
    "- 5 hidden layers of 6 neurons each\n",
    "\n",
    "We are not interested in successful optimization or loss convergence here, only training times. So we set `epochs=30` to speed things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train_neural_network(X, y, [6, 6], epochs=30, verbose=0)\n",
    "%timeit train_neural_network(X, y, [100, 100], epochs=30, verbose=0)\n",
    "%timeit train_neural_network(X, y, [6, 6, 6, 6, 6], epochs=30, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `[6, 6]` neural network is the fastest: this makes sense since it's the smallest.\n",
    "The `[100, 100]` model isn't that much slower however. Moreover, the `[6, 6, 6, 6, 6]` model is the slowest! This might be surprising when looking at the number of model parameters. We're feeling lazy so we'll let keras do the linear algebra, and use `.summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_neural_network([6, 6]).summary())\n",
    "print(create_neural_network([100, 100]).summary())\n",
    "print(create_neural_network([6, 6, 6, 6, 6]).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, training 10701 model weights is faster than training 205! It seems like _layers_ contribute more to overall training time than the number of hidden cells.\n",
    "\n",
    "Indeed, training time is most impacted by the _structure_ of the neural network. This is not only because deeper layer derivatives require more chained operations using _backpropagation_ , but also because many same layer calculations are _parallelised_. Neural network optimization is a complex problem with many different approaches and solutions, including algorithmic improvements, random methods, hardware solutions, and many more discovered every _week_. ðŸ¤¯\n",
    "\n",
    "Of course, we don't just design neural network architectures because of speed, as the structure has a big effect on model accuracy and overfitting. More on that next chapter!\n",
    "\n",
    "Next lecture we will go over basic improvements to gradient descent which can help train our models _faster_. ðŸŽ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercises\n",
    "\n",
    "ðŸ’ªðŸ’ªðŸ’ª Train a neural network regressor on the instagram planning dataset. Some helper functions are supplied so you can visualize the results after training ðŸ˜Ž. Here's a list of the steps you should be taking to lead your analysis:\n",
    "\n",
    "- load the `instagram_planning_norm.csv` dataset into a DataFrame\n",
    "- optionally visualize this dataset to refresh your memory\n",
    "- create a feature matrix, X, and a label vector, y\n",
    "- no need to standardize the features as they are already scaled \n",
    "- create a neural architecture with the Sequential api under the variable `model`\n",
    "- recommended: 1 hidden layer with 6 neurons, and a `relu` activation\n",
    "- think of the input dimensions of your feature vectors\n",
    "- think of the size and activation of your output layer: you are solving a regression task\n",
    "- compile the model with `optimizer='adam'` and `loss='mean_squared_error'`\n",
    "- fit the model with `batch_size=len(X)`\n",
    "- depending on your hidden layer activation function, you will need > 5000 `epochs` for the loss to converge.\n",
    "- store the output of `.fit()` in a variable called history\n",
    "- unit test your training with the provided code cell\n",
    "- visualize the predictions and loss curve with the second provided code cell\n",
    "\n",
    "ðŸ§  Why are we using a `mean_squared_error` loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def test_neural_network():\n",
    "    assert model, \"Couldn't find model\"\n",
    "    assert history, \"Couldn't find training history\"\n",
    "    assert len(history.history['loss']) > 10, f\"You only trained your model for {len(history.history)} epochs. Are you sure that's enough?\"\n",
    "    loss = history.history['loss'][-1]\n",
    "    assert loss < 15, f\"Your loss is {loss}, but it could be lower\"\n",
    "    print('Success! ðŸŽ‰')\n",
    "    \n",
    "test_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plot import plot_regression\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def model_viz(X, y, model, history):\n",
    "    fig = plt.figure(figsize=(12,5), dpi=120)\n",
    "\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    plot_regression(ax1, X, y, model)\n",
    "    ax1.set\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.set_title('Loss Curve');\n",
    "    \n",
    "model_viz(X, y, model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’ªðŸ’ª Feel free to explore more architectures and try new hyperparameters! You can always use the cell above to visualize their effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Today we learned about **neural network optimization**. First, we revisited cost functions and **gradient descent** in the context of a neural network. We then explained how the **chain rule** can calculate derivatives of the cost function with respect to weights of the first few layers. We showed how gradients can be calculated this way by **stepping** through the network in the **reverse** direction. We defined **backpropagation** as the optimization algorithm which computes the gradients. It **caches** and reuses derivative terms to make the calculation more efficient. We could then **define** neural networks as **nested non-linear functions** structured as **layered neurons**, whose weights are optimized using **gradient descent** and **backpropagation**. Finally, we trained a neural network classifier and regressor and analysed their loss curves.\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "- [3Blue1Brown - deep learning calculus](https://youtu.be/tIeHLnjs5U8)  \n",
    "Amazing channel with an outstanding video which derives gradient descent for neural networks. The whole series is excellent.\n",
    "\n",
    "## Additional Resources\n",
    "- [Why ReLU is better than other activation functions?](https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions)  \n",
    "Stackexchange thread explaining the popularity of ReLU as neural activation function\n",
    "- [thinc backpropagation](https://thinc.ai/docs/backprop101)  \n",
    "Alternative approach to explaining backpropagation, from a software engineering perspective\n",
    "- [Interpreting the loss curve](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic)  \n",
    "Typical loss curve profiles and how to interpret them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
