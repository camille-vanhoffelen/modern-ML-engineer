{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"computer_vision_pt.2.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CMofVoQMRDuc","colab_type":"text"},"source":["# Lecture 3.16: Computer Vision Pt. 2\n","\n","[**Lecture Slides**](https://docs.google.com/presentation/d/11OZUT30pmz3N45cch9wzwjYZy-Xn46Fsk5q_QR0dtYU/edit?usp=sharing)\n","\n","This lecture, we are going to fine-tune a deep Convolutional Neural Network (CNN) in pytorch.\n","\n","**Learning goals:**\n","- classify images using a pre-trained model from torchvision\n","- preprocess images with `torchvision.transforms`\n","- differentiate between `model.train()` and `model.eval()` modes\n","- apply image data augmentation\n","- fine-tune a pre-trained model to a binary image classification task\n","- predict using a fine-tuned model"]},{"cell_type":"markdown","metadata":{"id":"l2Da54tMRDud","colab_type":"text"},"source":["## 1. Introduction\n","\n","We seek to soothe these times of uncertainty with a sprinkle of sweetness. âœ¨ We want to open a dessert shop. We are now faced with a difficult decision: waffle ðŸ§‡, or ice cream ðŸ¦ ? These two treats are unequivocally delicious, but which is the most irresistible? This critical choice warrants some market research. We turn to the internet, but closer inspection of instagram posts reveals _millions_ of mouth-watering pictures to sift through. It's difficult to assess what the world needs most right now! \n","\n","To help analyse these enticing images, we decide to channel the power of _computer vision_. âš¡ï¸ In particular, we learned how modern CV machine learning models are _pre-trained_ on ImageNet, a large image classification dataset. These can then be _fine-tuned_ to solve specific tasks, with _transfer learning_. Let's try it out, and create a waffle/ice cream image classifier ðŸ´\n","\n","To ensure that this notebook can run on any hardware, locally or on the cloud, we assign the correct `device` to pytorch:"]},{"cell_type":"code","metadata":{"id":"WHD0D7HHRDue","colab_type":"code","colab":{}},"source":["import torch\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGrWFk_jlyIn","colab_type":"text"},"source":["This notebook will extensively use the [`torchvision`](https://github.com/pytorch/vision) library. It is maintained by pytorch, and according to them:\n","\n","> The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n","\n","The dependency was added to this repo's `Pipfile`, so if running locally, remember to execute:\n","\n","    pipenv install"]},{"cell_type":"markdown","metadata":{"id":"4bF0T_-olozx","colab_type":"text"},"source":["## 2. ImageNet Predictions\n","\n","###Â 2.1 Dogs\n","\n","Before we jump into fine-tuning and optimization, let's try to use a state-of-the-art model as is. Most of these models were pre-trained on ImageNet with 1000 classes. This should be broad enough to classify simple common images, for example a _dog_.\n","\n","We choose [ResNet-50](https://arxiv.org/abs/1512.03385) (although you could try any other!). `torchvision` makes it easy to download the neural network and its pre-trained weights:\n","\n"]},{"cell_type":"code","metadata":{"id":"Qg0zB1W2lt_V","colab_type":"code","colab":{}},"source":["from torchvision import models\n","\n","model = models.resnet50(pretrained=True)\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWDXWFuSmfKf","colab_type":"text"},"source":["ðŸ§ ðŸ§  Can you list what's special about the ResNet architecture?  (this [blogpost](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e4b1) might help)\n","\n","ResNet-50 is the first _deep_ learning model of this course. 50 stands for 50 layers, and we can count its parameters:"]},{"cell_type":"code","metadata":{"id":"s1X-4R32DEgq","colab_type":"code","colab":{}},"source":["sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Skf5HySlEAXs","colab_type":"text"},"source":["25.6 _million_ parameters ðŸ¤¯ Good thing we don't need to train this beast from scratch!\n","\n","We want to test the ResNet-50, but we need an image to classify first. This adorable dog will do nicely:"]},{"cell_type":"code","metadata":{"id":"3Re2AURZmj-q","colab_type":"code","colab":{}},"source":["from PIL import Image\n","!wget --quiet https://github.com/pytorch/hub/raw/master/dog.jpg\n","dog_img = Image.open('dog.jpg')\n","print(f'Image size: {dog_img.size}')\n","dog_img"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHhW5YCGnVPP","colab_type":"text"},"source":["Before we can feed this cutie to the ResNet-50 monster however, it needs to be converted into a pytorch `Tensor`.\n","\n","Last lecture, we manually transformed images into `ndarray`s. The task was manageable because our all our inputs were already 64x64 grayscale icons. This time, the sizes are inconsistent:\n","- most `torchvision` pre-trained models require 3x224x224  inputs\n","- our doggo is a 3x1546x1213 image\n","\n","Since we don't want to implement a cropping function ourselves, we can use `torchvision`'s transform package to help out. It provides a convenient `CenterCrop` class to reduce our doggo to a 224x224 image:"]},{"cell_type":"code","metadata":{"id":"E8t8uIQUH5Vm","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","\n","transforms.CenterCrop(224)(dog_img)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVmwq1VqIX4c","colab_type":"text"},"source":["Ah. ðŸ˜‘ This might be tricky to classify as a dog... Instead, we can first _resize_ the image, and then _crop_ small edges to make it square:"]},{"cell_type":"code","metadata":{"id":"2KVz3DLIIXQ8","colab_type":"code","colab":{}},"source":["dog_img_resize = transforms.Resize(256)(dog_img)\n","transforms.CenterCrop(224)(dog_img_resize)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UbyHiFDAH5vm","colab_type":"text"},"source":["What a good dog! ðŸ¶ \n","\n","On top of this resizing, `torchvision` models are trained on a _normalized_ version of ImageNet, meaning that we must _feature scale_ our inputs. This can be done with `transforms.Normalize()`, with the channel means and standard deviations specified in the [documentation](https://pytorch.org/docs/stable/torchvision/models.html#classification). \n","\n","However chaining all these transforms can get messy, so in the spirit of sklearn's `Pipeline`, we stack them in a [`Compose`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose) class:"]},{"cell_type":"code","metadata":{"id":"51ZGQmCvnsYa","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","input_tensor = preprocess(dog_img)\n","input_tensor.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rncjtxPpJ3vQ","colab_type":"text"},"source":["`transforms.ToTensor` even did the `ndarray` to `Tensor` conversion for us! All there is left to do is to reshape the `Tensor` , and send it to the `device`:"]},{"cell_type":"code","metadata":{"id":"BImhj3XCn5Ed","colab_type":"code","colab":{}},"source":["input_batch = input_tensor.unsqueeze(0)\n","input_batch = input_batch.to(device)\n","print(input_batch.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NslG97zGn5x-","colab_type":"text"},"source":["ðŸ§  What do the dimensions in our 1x3x224x224 input represent?\n","\n","Our data is ready, but there's one last thing to take care of before prediction.\n","\n","Complex models often use advanced regularization methods, such a [batch normalization](https://youtu.be/nUUqwaxLnWs) or [dropout](https://youtu.be/ARq74QuavAo). These affect the neural network _optimization_ , but aren't welcome during _prediction_. Instead of manually deactivating these behaviours, the `nn.Module` base class provides a method to switch between training & evaluation modes: `model.train()` and `model.eval()`.\n","\n","To make sure that our Resnet-50 classifies our dog image correctly, we have to switch the model to evaluation mode. We also use `torch.no_grad()` to prevent pytorch from tracking unnecessary `.grad_fn`, and we're good to go!"]},{"cell_type":"code","metadata":{"id":"KbtppwXwoXq6","colab_type":"code","colab":{}},"source":["model.eval()\n","with torch.no_grad():\n","  output = model(input_batch)\n","  print(output.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aM9QVhsyoYES","colab_type":"text"},"source":["1000 outputs? ðŸ¤” that's because of the 1000 ImageNet classes this model was trained on. Just like last lecture's CNN, the `torchvision` ResNet-50 used `CrossEntropyLoss` for improved numerical stability. To turn these outputs into probabilities, we must therefore apply a _softmax_. To turn these probabilities into predictions, we must follow with an _argmax_ operation to pick the most likely class."]},{"cell_type":"code","metadata":{"id":"jjHy83HTm19M","colab_type":"code","colab":{}},"source":["probas = torch.softmax(output, dim=1)\n","prob, pred = torch.max(probas, dim=1)\n","print(f'ResNet-50 prediction: {pred.item()}, probability: {prob.item()}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-sJhPF5rolxZ","colab_type":"text"},"source":["Our model is 87% confident about its choice... but what's a `258` though? ðŸ¤¨ \n","\n","we have to convert this index to its class name! Let's use a ready-made json dictionary so we don't have to download all of ImageNet."]},{"cell_type":"code","metadata":{"id":"5ZMxao3hpsEG","colab_type":"code","colab":{}},"source":["!wget --quiet https://raw.githubusercontent.com/raghakot/keras-vis/master/resources/imagenet_class_index.json\n","\n","import json\n","\n","with open('imagenet_class_index.json') as f:\n","  class_dict = json.load(f)\n","print(class_dict[\"99\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iEz99OLtIgNV","colab_type":"text"},"source":["Index `99` is a `goose`, and index `258` is ..."]},{"cell_type":"code","metadata":{"id":"RXCHJHd_p7Fs","colab_type":"code","colab":{}},"source":["pred_string = class_dict[str(pred.item())][1]\n","print(f'ResNet-50 prediction: {pred_string}, probability: {prob.item()}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rv0len5RrGlH","colab_type":"text"},"source":["A Samoyed! A quick google search confirms that not only is a Samoyed a dog, it's the exact breed shown in the picture! ðŸ•"]},{"cell_type":"markdown","metadata":{"id":"U4ZxTVr6sNyv","colab_type":"text"},"source":["### 2.2 Ice cream\n","\n","Classifying a dog is pretty impressive, but we're interested in ice cream and waffles. A `waffle_or_ice_cream` dataset is available on the course's S3 bucket, so we download it and extract its contents:"]},{"cell_type":"code","metadata":{"id":"ophetp69y1XD","colab_type":"code","colab":{}},"source":["!wget --quiet https://introduction-to-machine-learning-ilia-university.s3.eu-west-2.amazonaws.com/waffle_or_ice_cream.tar.gz\n","!tar -xf waffle_or_ice_cream.tar.gz\n","!ls waffle_or_ice_cream"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMf4nujQI_i2","colab_type":"text"},"source":["Two directories, `train` and `test`. Since we're interested in prediction, let's leave the `train` folder aside for now, and test our ResNet-50 on this ice cream picture:"]},{"cell_type":"code","metadata":{"id":"BkDajLiUsV2e","colab_type":"code","colab":{}},"source":["ice_cream_path = 'waffle_or_ice_cream/test/ice_cream.jpg'\n","Image.open(ice_cream_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtvhkeE2uPLH","colab_type":"text"},"source":["To avoid copy pasting code everywhere,  we wrapped the preprocessing and prediction in a function. Also we added a `print_top_predictions` function to return the top n probabilities of the network outputs:"]},{"cell_type":"code","metadata":{"id":"kJz5pJ-_rU8a","colab_type":"code","colab":{}},"source":["import os\n","\n","def predict_imagenet(path, model, top_n=None):\n","  input_image = Image.open(path)\n","  preprocess = transforms.Compose([\n","      transforms.Resize(256),\n","      transforms.CenterCrop(224),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","  ])\n","  input_tensor = preprocess(input_image)\n","  input_batch = input_tensor.unsqueeze(0)\n","  input_batch = input_batch.to(device)\n","\n","  model.eval()\n","  with torch.no_grad():\n","    outputs = model(input_batch)\n","    probas = torch.nn.functional.softmax(outputs, dim=1)\n","    prob, index = torch.max(probas, 1)\n","    pred = class_dict[str(index.item())][1]\n","    filename = os.path.basename(path)\n","    print(f'ResNet-50 file: {filename}, prediction: {pred}, probability: {prob.item()}')\n","    if top_n:\n","      print_top_predictions(probas[0], top_n)\n","    return input_image\n","  \n","\n","def print_top_predictions(probas, top_n):\n","  probas, indices = torch.sort(probas, descending=True)\n","  sorted_probas_indices = list(zip(probas, indices))\n","  print(f'top {top_n} predictions:')\n","  for prob, index in sorted_probas_indices[:top_n]:\n","    pred = get_class_name_imagenet(index.item())\n","    print(f'prediction: {pred}, probability: {prob.item()}')\n","\n","def get_class_name_imagenet(index):\n","  return class_dict[str(index)][1]\n","\n","predict_imagenet(ice_cream_path, model, 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b9YmvG4JesS","colab_type":"text"},"source":["99% ice cream! ResNet-50 is showing some promise, and might be useful for our market research. ðŸ˜\n","\n","ðŸ§  Can you describe how the `print_top_predictions()` function works?"]},{"cell_type":"markdown","metadata":{"id":"vSLxOtbitPO6","colab_type":"text"},"source":["###Â 2.3 Waffles\n","\n","Let's check if the model is also fond of waffles."]},{"cell_type":"code","metadata":{"id":"wZBJXqKctOok","colab_type":"code","colab":{}},"source":["waffle_path = 'waffle_or_ice_cream/test/waffle.jpg'\n","predict_imagenet(waffle_path, model, 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kbPhWyCAsHye","colab_type":"text"},"source":["... `waffle_iron`? Close, but it's not quite what we're looking for. If you check the [list of ImageNet classes](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a),  you'll find that there is no `waffle` class ... only `waffle_iron`! Even though there is no iron in the picture, the model has clearly _associated_ the waffle visuals with this class. On one hand, this is an interesting insight into the limitations of A.I. One the other, it means that we can't use this model for our market research. ðŸ˜“\n","\n","This is typical of large pre-trained models. They are optimized on academic datasets, and can rarely be used out of the box. This is when _fine-tuning_ comes in. ðŸ˜Ž"]},{"cell_type":"markdown","metadata":{"id":"0XgT1koqRDuh","colab_type":"text"},"source":["## 3. Fine-tuning\n","\n","Let's try to fine-tune ResNet-50 with our `waffle_or_ice_cream` dataset.\n","\n","### 3.1 Data Munging\n"]},{"cell_type":"code","metadata":{"id":"Z5dJeqBVRDun","colab_type":"code","colab":{}},"source":["!ls waffle_or_ice_cream/train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qh9QuZUSRDut","colab_type":"text"},"source":["Inside the `train` directory, we find an `ice_cream` and a `waffle` folder. It is common to have different image classes split into named directories. So common in fact, that `torchvision` includes a `Dataset` implementation just for this usecase. Instead of manually loading the images and giving them labels, we simply pass the main directory as argument to [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder):"]},{"cell_type":"code","metadata":{"id":"17QDVcC8RDut","colab_type":"code","colab":{}},"source":["from torchvision import datasets\n","\n","train_dir = 'waffle_or_ice_cream/train'\n","img_dataset = datasets.ImageFolder(train_dir)\n","img_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5f0PUeulL8He","colab_type":"text"},"source":["The `ImageFolder` has found the 694 images, but hasn't loaded them in memory yet. This is a python `iterable`, which we can use in a for-loop, or turn into an `iterator`:"]},{"cell_type":"code","metadata":{"id":"Fcm0k-GWRDuz","colab_type":"code","colab":{}},"source":["first_element = next(iter(img_dataset))\n","first_element"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZ-55QRSRDu1","colab_type":"text"},"source":["`ImageFolder` elements are typles of features (image) & labels (integer class index). It also stores the string class names into a list:"]},{"cell_type":"code","metadata":{"id":"sk4HDMK9RDu2","colab_type":"code","colab":{}},"source":["class_names = img_dataset.classes\n","class_names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nUNQ-tLRDu5","colab_type":"text"},"source":["Which allows us to format an example as such:"]},{"cell_type":"code","metadata":{"id":"55oxBGsrRDu6","colab_type":"code","colab":{}},"source":["img1, label1 = first_element\n","print('First example')\n","print(f'Class: {img_dataset.classes[label1]}')\n","img1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NffoBvoVRDu9","colab_type":"text"},"source":["`torchvision` pre-trained models expect normalized 3x224x224 images, but our ice creams and waffles are of many different shapes, sizes, and scales. Last section, we used `torchvision.transforms` to simplify the image preprocessing. We'll do the same for this training dataset, with an added twist.\n","\n","There are only 700 data points in our waffle or ice cream dataset. We wish to _augment_ our dataset by adding _randomness_. ðŸŽ² Once again, `torchvision` offers two transforms to make this easy:\n","- [`RandomResizedCrop()`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomResizedCrop)\n","- [`RandomHorizontalFlip()`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip)\n","\n","They both work as their names suggest, but feel free to check out the documentation for more details (and see [albumentations](https://github.com/albumentations-team/albumentations) for more advanced image augmentation).\n","\n","We can stack these two transforms as part of our preprocessing pipeline:"]},{"cell_type":"code","metadata":{"id":"gjDYd3JnRDu-","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","\n","train_preprocess = transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8jZ-QHMPLnL","colab_type":"text"},"source":["And then apply our `train_preprocess` directly into the `ImageFolder`. Neat! ðŸ‘Œ"]},{"cell_type":"code","metadata":{"id":"cIkigEkcRDvA","colab_type":"code","colab":{}},"source":["img_dataset = datasets.ImageFolder(train_dir, train_preprocess)\n","img_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMVtXy4aRDvD","colab_type":"text"},"source":["Despite the randomness, the length of our dataset is still 695. This is because the randomness is applied _lazily_ as we step through the examples.\n","\n","`Dataset`s feed in nicely with `DataLoader`s, which means our training data is ready for fine-tuning:"]},{"cell_type":"code","metadata":{"id":"OSStT97_RDvD","colab_type":"code","colab":{}},"source":["dataloader = torch.utils.data.DataLoader(img_dataset, batch_size=4, shuffle=True, num_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m-RhDhpWRDvG","colab_type":"text"},"source":["### 3.2 Optimization\n","\n","We could reuse the ResNet-50 from earlier in this notebook, but we chose to downgrade to ResNet-18, which \"only\" has 18 layers. This is because our problem and dataset size is similar to the [bees and ants](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) fine-tuning tutorial, and they use `resnet18`. Typically, using very large models with small datasets leads to overfitting, so going big isn't always the right choice."]},{"cell_type":"code","metadata":{"id":"cY-R4pd0RDvG","colab_type":"code","colab":{}},"source":["from torchvision import models\n","\n","model_ft = models.resnet18(pretrained=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_USXtOT_RDvI","colab_type":"text"},"source":["Before we can fine-tune this neural network, we have to adapt the last classification layer to our task. \n","We know that ResNet (and CNNs in general) use a _fully-connected layer_ as final output layer. Pre-trained `torchvision` models were optimized on 1000 class ImageNet classification, so their last layer has 1000 outputs. We are dealing with binary classification, so we must replace it with a 1 neuron output layer.\n","\n","`nn.Linear` requires two arguments, the input dimension, and the output dimension. We need to know how many outputs are returned by the penultimate layer to construct our new `nn.Linear`. This is done by returning the `.in_features` field (input features) of the `fc` field (fully connected), of our `model_ft`."]},{"cell_type":"code","metadata":{"id":"_UVyEXr6RDvJ","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","num_ftrs = model_ft.fc.in_features\n","num_ftrs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-P79JQySHvw","colab_type":"text"},"source":["This final layer maps 512 activations to the outputs, so in the case of our waffle/ice-cream classification, 512 inputs to 1 output. We can replace the model's fully connected layer directly ðŸ”„:"]},{"cell_type":"code","metadata":{"id":"EFM-ab-iSHTy","colab_type":"code","colab":{}},"source":["model_ft.fc = nn.Linear(num_ftrs, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wFYqA681RDvN","colab_type":"text"},"source":["ðŸ§  Take the time to understand how we replaced the ResNet-18's last layer, and _why_.\n","\n","Let's not forget to send the model parameters to the correct `device`:"]},{"cell_type":"code","metadata":{"id":"Q-2BLxoTRDvO","colab_type":"code","colab":{}},"source":["model_ft = model_ft.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kyMDr2MfSsiy","colab_type":"text"},"source":["We'll use the numerically stable `BCEWithLogitsLoss`, and a simple `SGD` optimizer with momentum."]},{"cell_type":"code","metadata":{"id":"iaxxGs1LRDvS","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z86V3DgLWqlr","colab_type":"text"},"source":["The two new additions to our training loop are:\n","- `model.train()` to activate ResNet's batch normalization, acts like the opposite of `model.eval()`\n","- `with torch.set_grad_enabled(True):` necessary to _unfreeze_ the pretrained model weights, acts like the opposite of `with torch.no_grad()`"]},{"cell_type":"code","metadata":{"id":"l9cSutHqRDvU","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","\n","def train_model(model, criterion, optimizer, num_epochs=20):\n","\n","    torch.manual_seed(1337)\n","    np.random.seed(666)\n","\n","    start_time = time.time()\n","    epoch_losses = []\n","\n","    # enable batch normalization\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        running_losses = []\n","\n","        for inputs, labels in dataloader:\n","            inputs = inputs.to(device)\n","            # pytorch likes floats\n","            labels = labels.float().unsqueeze(1).to(device)\n","            optimizer.zero_grad()\n","            # unfreeze pretrained model\n","            with torch.set_grad_enabled(True):\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","            running_losses.append(loss.item())\n","        \n","        epoch_loss = np.array(running_losses).mean()\n","        epoch_losses.append(epoch_loss)\n","        print(f'epoch: {epoch}, loss: {epoch_loss:4f}')\n","\n","    training_time = time.time() - start_time\n","    print(f'Training complete in {training_time//60:.0f}m {training_time%60:.0f}s')\n","    \n","    return model, epoch_losses"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOnaT9CWfXy3","colab_type":"text"},"source":["We are ready to train! ðŸ‹ï¸â€â™€ï¸"]},{"cell_type":"code","metadata":{"id":"X-jPIPSpRDvW","colab_type":"code","colab":{}},"source":["model_ft, epoch_losses = train_model(model_ft, criterion, optimizer_ft, num_epochs=20)\n","\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(dpi=120)\n","ax = fig.add_subplot()\n","ax.plot(epoch_losses)\n","ax.set_ylabel('loss')\n","ax.set_xlabel('epochs')\n","ax.set_title('Loss Curve');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5q6bhGVqRDvY","colab_type":"text"},"source":["The loss decreases, but it's not the prettiest optimization. ðŸ˜ That's often the deal with the highly non-convex loss surfaces of deep learning!\n","\n","We _are_ however using regular mini-batch gradient descent with momentum. We can do better than that! The loss is very _unstable_ towards the end of training, which might suggest that the learning rate is too high.\n","\n","ðŸ§  Why can a high learning rate prevent the loss from converging?\n","\n","We learned in lecture 3.13 that _learning rate decay_ can help with this problem. In pytorch, this can be done with a learning rate scheduler, such as [`StepLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR). \n","\n","ðŸ’ªðŸ’ª Fine-tune ResNet-18 with learning rate decay.\n","- rewrite the `train_model()` to also include a `StepLR`. check the [documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) for more details.\n","- try hyperparameters `step_size=4`, `gamma=0.2` (or others!)\n","- reload a `resnet18` model from `torchvision`. Don't forget to modify the last layer\n","- train this new model\n","- plot the loss curve"]},{"cell_type":"code","metadata":{"id":"sv8TR3psbysO","colab_type":"code","colab":{}},"source":["# INSERT YOUR CODE HERE\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yCgnZzOMeZ6Q","colab_type":"text"},"source":["ðŸ§  Has the loss curve improved? Is this what you expected? \n","\n","ðŸ§ ðŸ§  What other factors can explain the variance of this loss curve?"]},{"cell_type":"markdown","metadata":{"id":"ZVc59ikcRDva","colab_type":"text"},"source":["## 4. Waffle Or Ice Cream Prediction\n","\n","Now that we have fine-tuned our ResNet-18 model, let's check if it fares better than ResNet-50 on our classification task. We rewrite a `predict()` function inspired by that of section 2, with a few differences.\n","\n","ðŸ§  Why are we using `torch.sigmoid()` instead of `torch.softmax()` here?\n","\n","ðŸ§  Can you explain how the `get_class_name()` function works?"]},{"cell_type":"code","metadata":{"id":"A3UPn0lLYghP","colab_type":"code","colab":{}},"source":["import os\n","\n","def predict_waffle_or_ice_cream(path, model):\n","  input_image = Image.open(path)\n","  preprocess = transforms.Compose([\n","      transforms.Resize(256),\n","      transforms.CenterCrop(224),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","  ])\n","  input_tensor = preprocess(input_image)\n","  input_batch = input_tensor.unsqueeze(0)\n","  input_batch = input_batch.to(device)\n","\n","  model.eval()\n","  with torch.no_grad():\n","    outputs = model(input_batch)\n","    prob = torch.sigmoid(outputs).item()\n","    pred = get_class_name(prob)\n","    filename = os.path.basename(path)\n","    print(f'ResNet-50 file: {filename}, prediction: {pred}, waffle probability: {prob:.4f}')\n","  return input_image\n","\n","def get_class_name(prob):\n","  class_names = img_dataset.classes\n","  return class_names[1] if prob > 0.5 else class_names[0]\n","\n","ice_cream_path = 'waffle_or_ice_cream/test/ice_cream.jpg'\n","predict_waffle_or_ice_cream(ice_cream_path, model_ft)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-htdRagkRDvn","colab_type":"code","colab":{}},"source":["waffle_path = 'waffle_or_ice_cream/test/waffle.jpg'\n","predict_waffle_or_ice_cream(waffle_path, model_ft)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zGPJ5uuRDvn","colab_type":"text"},"source":["Our model has managed to correctly classify both the ice cream and the waffle!\n","\n","This was possible because we fine-tuned ResNet-18, and leveraged its pre-trained knowledge. Without transfer learning, this task would have taken more resources, more time, and more data.\n","\n","So, are we ready to take over the world of desserts? ðŸ¨ Some examples aren't as simple as the two images above. No model is perfect, and there are almost always exceptions to the classes we're trying to identify. For our waffle or ice cream problem, let's introduce:  \n","âœ¨ the waffle cone âœ¨\n","\n"]},{"cell_type":"code","metadata":{"id":"sVLIYl9nXMAg","colab_type":"code","colab":{}},"source":["waffle_path = 'waffle_or_ice_cream/test/waffle_ice_cream.jpg'\n","predict_waffle_or_ice_cream(waffle_path, model_ft)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSS_KmnpRDvp","colab_type":"text"},"source":["ðŸ§  what was the waffle cone prediction? do you agree with it?\n","\n","This image is confusing for the model ... and for humans! ðŸ¤¨ In fact, if you retrain the model several times, you might end up with completely opposite results.\n","\n","ðŸ§ ðŸ§  If we were to use this classifier in a production setting, how would you remedy this instability in prediction? "]},{"cell_type":"markdown","metadata":{"id":"q9n4B5czvuMq","colab_type":"text"},"source":["## 5. Bonus\n","\n","We used the pre-trained ResNet-50 on a few dog, ice cream, and waffle images. But it's capable of predicting 1000 classes! You can use your laptop's webcam inside this notebook to take pictures and save them locally. ðŸ“¸ These can then be classified by the original pre-trained ResNet-50."]},{"cell_type":"code","metadata":{"id":"ZNFwqNQNNgmg","colab_type":"code","colab":{}},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5dW12tdNglp","colab_type":"code","colab":{}},"source":["import IPython\n","\n","try:\n","  filename = take_photo()\n","  print('Saved to {}'.format(filename))\n","  \n","  # Show the image which was just taken.\n","  display(IPython.display.Image(filename))\n","except Exception as err:\n","  # Errors will be thrown if the user does not have a webcam or if they do not\n","  # grant the page permission to access it.\n","  print(str(err))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBFxrl5xOupf","colab_type":"code","colab":{}},"source":["predict_imagenet('photo.jpg', model, 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5SWTlrI9pBIr","colab_type":"text"},"source":["Here's my attempt :) "]},{"cell_type":"code","metadata":{"id":"Ml6j6A5ta5eW","colab_type":"code","colab":{}},"source":["!wget --quiet https://introduction-to-machine-learning-ilia-university.s3.eu-west-2.amazonaws.com/wine_bottle.jpg\n","predict_imagenet('wine_bottle.jpg', model, 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CgyDOm8PZD45","colab_type":"text"},"source":["## 6. Summary\n","\n","Today we learned about **modern computer vision models** and **transfer learning**. We first revisited the **history of CNNs** to understand the improvements that led to the current state of the art. We pointed out a trend of **wider**, **deeper** models, and **larger** datasets, and noted that restricts most data scientists from training modern CV models from scratch. We explored the **hidden representations** of deep CNNs and understood the relationship between the **depth**, **specifity**, and **power** of the layers. After noting that large CNNs learned **similar** low-level features for each training, we defined **transfer learning** as techniques that **share** this information between models for similar tasks. We described two methods: **feature extraction** and **fine-tuning**, and showed how they can help create accurate CV models with **smaller datasets** and **less hardware**. We highlighted how this played into the unique machine learning **open-source** ecosystem, which democratises knowledge and tools to build cutting edge A.I systems. We reviewed a few pro-tips for CV, and explained how **data augmentation** can help further squeeze learnable information out of a dataset. Finally, we applied transfer learning to computer vision by fine-tuning a **ResNet-18** model pre-trained on ImageNet, to a waffle or ice cream image classification problem.\n","\n","# Resources\n","\n","## Core Resources\n","\n","- [Transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)  \n","Official pytorch tutorial that this notebook was based on\n","- [cs231n - transfer learning](https://cs231n.github.io/transfer-learning/)  \n","Succint and practical notes on transfer learning from Karpathy's classic computer vision course\n","- [torchvision](https://github.com/pytorch/vision)  \n","torchvision github repository\n","- [Waffle or ice cream dataset](https://www.kaggle.com/sapal6/waffles-or-icecream)  \n","Kaggle dataset used to make this notebook\n","\n","### Additional Resources\n","\n","- [Towards reprodubility with pytorch hub](https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/)  \n","Pytorch hub introduction\n","- [Finetuning torchvision models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)  \n","More in depth official pytorch tutorial showing how to fine-tune _all_ torchvision pre-trained models\n","- [CNN architectures](https://towardsdatascience.com/neural-network-architectures-156e5bad51ba)  \n","Breakdown of major CNN architectures.\n","- [Illustrated 10 CNN architectures](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d)  \n","Illustrated breakdown of CNN architectures\n","- [Feature visualization in NNs](https://distill.pub/2017/feature-visualization/)  \n","Article visually representing the hidden representations of GoogLeNet\n","- [albumentations](https://github.com/albumentations-team/albumentations)  \n","Image augmentation library, part of the pytorch ecosystem\n","- [The data that changed the world](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/)  \n","Quartz article about the disruptive effect of ImageNet on CV and A.I\n","- [Why does batch normalization work](https://youtu.be/nUUqwaxLnWs)  \n","Deeplearning.ai video providing intuition on why batch normalization works\n","- [ResNets](https://www.coursera.org/lecture/convolutional-neural-networks/resnets-HAhz9)  \n","deeplearning.ai lecture on ResNets and skip connections\n","- [Understanding dropout](https://youtu.be/ARq74QuavAo)  \n","deeplearning.ai video provising intuition on why dropout works "]},{"cell_type":"code","metadata":{"id":"XLZ5qYOpUDhc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}