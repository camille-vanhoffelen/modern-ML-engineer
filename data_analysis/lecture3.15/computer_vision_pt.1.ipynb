{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "093q8xLEPwRT"
   },
   "source": [
    "# Lecture 3.15: Computer Vision Pt. 1\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/1peBshx2Ift4UGNylVcC5pvroWx883h8mklruR8bEMzM/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to train a Convolutional Neural Network (CNN) in pytorch.\n",
    "\n",
    "**Learning goals:**\n",
    "- convert images to pytorch-read `Tensor`s using `pillow`\n",
    "- create a CNN\n",
    "- train a CNN\n",
    "- debug a CNN by printing out layer input/output sizes\n",
    "- plot a loss curve per epoch\n",
    "- understand how CNN width vs depth affects optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OisYtQEiXX89"
   },
   "source": [
    "##¬†1. Introduction\n",
    "\n",
    "\n",
    "This notebook can be run locally with jupyter, or on [Google colab](https://colab.research.google.com/github/camille-vanhoffelen/introduction-to-machine-learning/blob/master/data_analysis/lecture3.15/computer_vision_pt.1.ipynb). This is because we choose the right `device` below. Try to compare the training speeds of CPU & GPU runtimes! üèé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFRmnStQTIp9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NT8S1binRf_S"
   },
   "source": [
    "The [Unicode Consortium](https://en.wikipedia.org/wiki/Unicode_Consortium) has contacted us and needs our help. ‚òéÔ∏è They receive too many new emoji proposals, and cannot keep track of all of them. In order to choose the next emojis, they wish to _classify_ the submissions into three groups: `face`, `flag`, and `animal`.\n",
    "\n",
    "Let's make a Convolutional Neural Network to classify emojis. üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLZzjufsXlxw"
   },
   "source": [
    "## 2. Data Munging\n",
    "\n",
    "The consortium has provided a _training dataset_ emoji images. These are stored in a public [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html) bucket. We can download them to our local/cloud environment with [`wget`](https://www.gnu.org/software/wget/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SrFs-Khh3pA"
   },
   "outputs": [],
   "source": [
    "!wget https://introduction-to-machine-learning-ilia-university.s3.eu-west-2.amazonaws.com/emojis.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHmgmuJ0lcPG"
   },
   "source": [
    "The emojis are packaged in a [`tar`](https://en.wikipedia.org/wiki/Tar_(computing) compressed archive, which can be extracted with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5IJ4S3mh-I6"
   },
   "outputs": [],
   "source": [
    "!tar -xf emojis.tar.gz\n",
    "!ls emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-puup4tlrUU"
   },
   "source": [
    "The archive contains 2 directories, `test` & `train. For now, we are interested in the `train` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTvWZpPVnC6j"
   },
   "outputs": [],
   "source": [
    "!ls emojis/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kT1eElcnG8g"
   },
   "source": [
    "The training data is split in three directories corresponding to each class: `animals`, `faces`, `flags`.\n",
    "\n",
    "Let's have a look at the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcE3oT4hnYgX"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('emojis/train/faces/42.png')\n",
    "print(f'image size: {img.size}')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmw9IP0hnV8Y"
   },
   "source": [
    "It looks like we are dealing with 64x64 grayscale images.\n",
    "\n",
    "We wish to load these independently into 3 list of `ndarray`s, one for each class. Check out lecture 2.5 if you'd like a refresher on `pillow` and how to convert images to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4i0nHsmoPQW"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def load_imgs(input_dir):\n",
    "  paths = glob.glob(input_dir + '*.png')\n",
    "  imgs = [Image.open(path) for path in paths]\n",
    "  return [np.array(img).reshape(1, 64, 64) for img in imgs]\n",
    "\n",
    "\n",
    "faces_dir = \"emojis/train/faces/\"\n",
    "faces_features = load_imgs(faces_dir)\n",
    "\n",
    "flags_dir = \"emojis/train/flags/\"\n",
    "flags_features = load_imgs(flags_dir)\n",
    "\n",
    "animals_dir = \"emojis/train/animals/\"\n",
    "animals_features = load_imgs(animals_dir)\n",
    "\n",
    "faces_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnuBOfhJojNl"
   },
   "source": [
    "We created 1x64x64 `ndarrays` because convention is to place the [channel](https://en.wikipedia.org/wiki/Channel_(digital_image)) dimension before the height & width. Our images are grayscale, so this channel dimension is 1.\n",
    "\n",
    "We now wish to turn these _feature matrices_ into _examples_ by matching them with a _label_. To do so, we create integer labels corresponding to our classes:\n",
    "- faces: 0\n",
    "- flags: 1\n",
    "- animals: 2\n",
    "\n",
    "We then use [`.zip()`](https://realpython.com/python-zip-function/) ü§ê to combine the features and labels just like last lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLdy3XgqTIqC"
   },
   "outputs": [],
   "source": [
    "def label(features, class_index):\n",
    "  labels = np.full(len(features), class_index, dtype=np.int8)\n",
    "  return list(zip(features, labels))\n",
    "    \n",
    "faces_examples = label(faces_features, 0)\n",
    "flags_examples = label(flags_features, 1)\n",
    "animals_examples = label(animals_features, 2)\n",
    "\n",
    "examples = faces_examples + flags_examples + animals_examples\n",
    "\n",
    "print(f\"feature shape: {examples[0][0].shape}\")\n",
    "print(f\"label value: {examples[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWQQFc6QpWeg"
   },
   "source": [
    "We have turned the emojis into a collection of examples: pairs of feature matrices and labels. Let's get training! üèãÔ∏è‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqenolWSbmCs"
   },
   "source": [
    "## 3.Training\n",
    "\n",
    "### 3.1 CNN Architecture\n",
    "\n",
    "We are going to create a Convolutional Neural Network class called `ConvNet` with the same process as lecture 3.14. We extend the `nn.Module` and implement the `.forward()` method. Layers are initialized in the `ConvNet` `__init__()` constructor.\n",
    "\n",
    "Since we are _still_ lazy, we use pytorch's [`Conv2D`](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html) and [`MaxPool2D`](https://pytorch.org/docs/master/generated/torch.nn.MaxPool2d.html) layers. Familiarize yourself with their arguments: \n",
    "- `in_channels` is the _depth_ of the input volume\n",
    "- `out_channels` is the _depth_ of the output volume (# of filters)\n",
    "- `kernel_size` is the filter/field matrix size\n",
    "- you should recognize `stride` and `padding` from the lecture slides\n",
    "\n",
    "Fully connected layers use `nn.Linear` as in lecture 3.14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2osl1gZVGAq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        super(ConvNet, self).__init__()\n",
    "  \n",
    "        self.verbose = verbose\n",
    "        # 1x64x64 => 8x64x64\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)\n",
    "        # 8x64x64 => 8x32x32\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)\n",
    "        # 8x32x32 => 16x32x32\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=8,\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)\n",
    "        # 16x32x32 => 16x16x16                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)\n",
    "        \n",
    "        # 16x16x16 => 32x16x16\n",
    "        self.conv_3 = torch.nn.Conv2d(in_channels=16,\n",
    "                                      out_channels=32,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)\n",
    "        \n",
    "        # 16x16x32 => 8x8x32                             \n",
    "        self.pool_3 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)\n",
    "        \n",
    "        # 2048 => 64\n",
    "        self.linear_1 = torch.nn.Linear(8*8*32, 64)\n",
    "        # 64 => 3\n",
    "        self.linear_2 = torch.nn.Linear(64, 3)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "      x = F.relu(self.conv_1(x))\n",
    "      x = self.pool_1(x)\n",
    "\n",
    "      x = F.relu(self.conv_2(x))\n",
    "      x = self.pool_2(x)\n",
    "\n",
    "      x = F.relu(self.conv_3(x))\n",
    "      x = self.pool_3(x)\n",
    "      \n",
    "      # flatten\n",
    "      x = x.view(-1, 8*8*32)\n",
    "\n",
    "      x = F.relu(self.linear_1(x))\n",
    "      \n",
    "      logits = self.linear_2(x)\n",
    "      return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L24PjL9Ravoj"
   },
   "source": [
    "üß† Try to track the sizes of the neuron volumes throughout the convolutional network.\n",
    "\n",
    "üß† How do convolutional layers maintain their input's height & width? How do pooling layers half their input's height & width?\n",
    "\n",
    "Two things might stand out when looking at the `ConvNet` code above\n",
    "\n",
    "**flatten:**\n",
    "\n",
    "Notice the `# flatten` operation between the last pooling layer and the first fully connected layer. This is necessary because convolutional & pooling layers deal with input _volumes_, whereas dense layers operate on _vectors_. We therefore reshape the activations into a 1D `Tensor`, which is called _flattening_. This doesn't change the tensor values, only their spatial arrangement.\n",
    "\n",
    "**logits:**\n",
    "\n",
    "We've seen `logits` before but haven't formally defined them. [Logits](https://developers.google.com/machine-learning/glossary/#logits) are the unbounded outputs of a linear layer, that yet have to be fed into a sigmoid/softmax function. i.e they are scalar values that are to be transformed into probabilities. \n",
    "\n",
    "Last lecture, we used the [`BCEWithLogits`](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss) loss, which required returning the _logits_ as opposed to the sigmoid _probabilities_ as output of our neural network. For this multi-class classification problem, we use the [`CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) which works in the same way: we don't apply the softmax ourselves, it is already included in the loss to improve numerical stability. üßò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLQIHC6cVxfZ"
   },
   "source": [
    "### 3.2 Debugging\n",
    "\n",
    "Pytorch's dynamic computation graph makes neural network code easy to debug. One can step through the execution, and investigate outputs during runtime. \n",
    "\n",
    "To showcase this, we rewrote our CNN and added optional `print()` statements between every single layer. Creating a `ConvNet` with `verbose=True` will then print the tensor shapes flowing through the network.\n",
    "\n",
    "This is only for demonstration purposes and is a bad idea in general for two reasons:\n",
    "- the `ConvNet` code has become messy\n",
    "- once the code works, we don't need the option to print out shapes for every single `.forward()` pass\n",
    "\n",
    "Instead, data scientists typically use [debuggers](https://docs.python.org/3/library/pdb.html). But now we know that  we can add print statements everywhere if we feel like it!\n",
    " üò§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0Z3Afb6YWsY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        super(ConvNet, self).__init__()\n",
    "  \n",
    "        self.verbose = verbose\n",
    "        # 1x64x64 => 8x64x64\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)\n",
    "        # 8x64x64 => 8x32x32\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)\n",
    "        # 8x32x32x8 => 16x32x32\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=8,\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)\n",
    "        # 16x32x32 => 16x16x16                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)\n",
    "        \n",
    "        # 16x16x16 => 32x16x16\n",
    "        self.conv_3 = torch.nn.Conv2d(in_channels=16,\n",
    "                                      out_channels=32,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)\n",
    "        \n",
    "        # 16x16x32 => 8x8x32                             \n",
    "        self.pool_3 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)\n",
    "        \n",
    "        # 8x8x32 => 64\n",
    "        self.linear_1 = torch.nn.Linear(8*8*32, 64)\n",
    "        # 64 => 3\n",
    "        self.linear_2 = torch.nn.Linear(64, 3)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "      if self.verbose: \n",
    "        print(f\"conv_1 input: {repr(x.shape)}\")\n",
    "      x = F.relu(self.conv_1(x))\n",
    "      if self.verbose: \n",
    "        print(f\"conv_1 output: {x.shape}\")\n",
    "\n",
    "      x = self.pool_1(x)\n",
    "      if self.verbose: \n",
    "        print(f\"pool_1 output: {x.shape}\")\n",
    "\n",
    "      x = F.relu(self.conv_2(x))\n",
    "      if self.verbose: \n",
    "        print(f\"conv_2 output: {x.shape}\")\n",
    "\n",
    "      x = self.pool_2(x)\n",
    "      if self.verbose: \n",
    "        print(f\"pool_2 output: {x.shape}\")\n",
    "\n",
    "      x = F.relu(self.conv_3(x))\n",
    "      if self.verbose: \n",
    "        print(f\"conv_3 output: {x.shape}\")\n",
    "\n",
    "      x = self.pool_3(x)\n",
    "      if self.verbose: \n",
    "        print(f\"pool_3 output: {x.shape}\")\n",
    "        \n",
    "      x = F.relu(self.linear_1(x.view(-1, 8*8*32)))\n",
    "      if self.verbose: \n",
    "        print(f\"linear_1 output: {x.shape}\")\n",
    "\n",
    "      logits = self.linear_2(x)\n",
    "      if self.verbose: \n",
    "        print(f\"linear_2 output: {logits.shape}\")\n",
    "      return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vu6iiZH-xpSH"
   },
   "source": [
    "To test the verbose output, we fetch and convert the first example to a pytorch-ready `Tensor`. Notice the `Tensor` is reshaped to 1x1x64x64, the first dimension standing for the _batch size_ (which is 1 in our case). \n",
    "\n",
    "We use the `torch.no_grad()` context to let pytorch know that it doesn't need to keep track of `.grad_fn`, even if `Tensor`s were built with `requires_grad=True`. This speeds up predictions and lowers their memory consumption. Always use `torch.no_grad()` if you aren't planning to `.backward()` through the `Tensor`s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQw6JvsHxut1"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  features, label = examples[0]\n",
    "  features = torch.tensor(features).float().view(1, 1, 64, 64)\n",
    "  net = ConvNet(verbose=True)\n",
    "  net(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQuOS114IbH2"
   },
   "source": [
    "The print statements worked, and we can clearly see the effects of the convolutional and pooling layers on the activation volumes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_s0mR33LI4gU"
   },
   "source": [
    "### 3.3 Optimization\n",
    "\n",
    "Let's train our `ConvNet`! First, we create a `DataLoader` to shuffle and iterate through batches üîÄ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fb0Er4WPbsPV"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dataset=examples, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruVD-pNKYm24"
   },
   "source": [
    "We can then initialize our network, optimizer, and criterion. Notice that we're sending the network parameters to the runtime's `device`, whether this notebook is run on a CPU or a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSGxHN1nJVU6"
   },
   "outputs": [],
   "source": [
    "net = ConvNet()\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfmYYMZRJbki"
   },
   "source": [
    "We are now ready to loop through the dataset and train the model parameters. This code is almost the same as last lecture's banknote classifier, with the exception of:\n",
    "- `loss_per_epoch` is an example of how to keep track of the average loss for each epoch\n",
    "- `labels` have `dtype=long` because our `CrossEntropyLoss` expects categorical whole numbers; the `0`, `1`, & `2` corresponding to our classes (see [documentation](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html)). \n",
    "- `start_time` & `stop_time` make it easy to compare CPU and GPU training times, if this notebook is run in different environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJb7gAUebxSG"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(666)\n",
    "\n",
    "start_time = time.time()    \n",
    "loss_per_batch = []\n",
    "loss_per_epoch = []\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    running_losses = []\n",
    "    for batch, data in enumerate(data_loader):\n",
    "        \n",
    "        features, labels = data\n",
    "        features = features.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        logits = net(features)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss_per_batch.append(loss.item())\n",
    "        running_losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_epoch_loss = np.array(running_losses).mean()\n",
    "    print(f\"epoch: {epoch}, loss: {mean_epoch_loss:.6f}\")\n",
    "    loss_per_epoch.append(mean_epoch_loss)\n",
    "\n",
    "stop_time = time.time()\n",
    "print(f'total training time: {stop_time - start_time}')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvH84IgtLfy4"
   },
   "source": [
    "üß† Take the time to understand each step of the loop above. Check out lecture 3.14 for a refresher.\n",
    "\n",
    "Our CNN was trained! üï∫ It looks like the loss was successfully minimized. Let's visualize some loss curves to know more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vB0DHQCjcK0n"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(loss_per_batch, loss_per_epoch, ylim=(-0.1, 1)):\n",
    "  fig = plt.figure(figsize=(12, 4))\n",
    "  ax1 = fig.add_subplot(121)\n",
    "  ax1.plot(loss_per_batch)\n",
    "  ax1.set_ylim(ylim)\n",
    "  ax1.set_ylabel('loss')\n",
    "  ax1.set_xlabel('batch')\n",
    "  ax1.set_title('Loss Curve')\n",
    "\n",
    "  ax2 = fig.add_subplot(122)\n",
    "  ax2.plot(loss_per_epoch)\n",
    "  ax2.set_ylim(ylim)\n",
    "  ax2.set_ylabel('loss')\n",
    "  ax2.set_xlabel('epoch')\n",
    "  ax2.set_title('Loss Curve')\n",
    "\n",
    "plot_loss_curves(loss_per_batch, loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zM3LYmHILjo8"
   },
   "source": [
    "The loss curve per _epoch_ displays the same information as the per _batch_ graph, except the values were averaged across batches. This should be clear from the training loop code above (see `running_losses`). Loss curves are typically shown per epoch for deep neural networks, as these can routinely take > 100 epochs to converge.\n",
    "\n",
    "The curves suggest that our model was trained successfully. üòé\n",
    "\n",
    "üß† Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD1PkQdFZ3wc"
   },
   "source": [
    "### 3.4 Exercises\n",
    "\n",
    "We are curious about CNN architectures... there are so many ways of arranging and sizing the different layers! In particular, we want to know about the power of _width_ vs _depth_. If we put all 32 filters in a single layer, how will this affect the neural network's optimization? ü§î\n",
    "\n",
    "üí™üí™üí™ Train a convolutional net with a single convolutional + pooling layer pair, and plot its loss curve.\n",
    "- create a new CNN class called `WideConvNet`\n",
    "- use one convolutional layer with 32 filters, followed by a standard pooling layer\n",
    "- adjust the input the size of your fully connected layer accordingly\n",
    "- remember to update both the `__init__` and `.forward()` methods\n",
    "- create an instace of `WideConvNet` called `wide_net` (or this will affect section 4!)\n",
    "- train the `wide_net` with the same training loop as above\n",
    "- create `loss_per_batch` and `loss_per_epoch` to use the plotting method run before the unit test\n",
    "- tip: keep track of the shapes of the volumes flowing between the layers. You can always print them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh7BeTYNZzm-"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-NxSaBWT3lp"
   },
   "outputs": [],
   "source": [
    "def test_wide_conv_net():\n",
    "  with torch.no_grad():\n",
    "    wide_net = WideConvNet()\n",
    "    named_params = list(wide_net.named_parameters())\n",
    "    n_layers = len(named_params)\n",
    "    assert n_layers == 6, f\"Expected 6 layers, but got {n_layers}. Are you using 1 conv, 1 pool, and 2 linear layers?\"\n",
    "    n_filters = named_params[1][1].size()[0]\n",
    "    assert n_filters == 32 , f\"Expected 32 convolutional filters, but got {n_filters}\"\n",
    "    linear_1_inputs = named_params[2][1].size()[1]\n",
    "    assert linear_1_inputs == 32768 , f\"Expected 32768 inputs for linear_1, but got {linear_1_inputs}\"\n",
    "    print('Success! üéâ')\n",
    "\n",
    "plot_loss_curves(loss_per_batch, loss_per_epoch, ylim=(-0.1, 10))\n",
    "test_wide_conv_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78d9Xy6obZ-N"
   },
   "source": [
    "üß†üß† The loss curve is significantly different to our first CNN architecture. What changed? What does this suggest about this trained model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rBJ_D_jKNBr"
   },
   "source": [
    "## 4. Prediction\n",
    "\n",
    "Our CNN is trained, and the loss function converged, but we aren't quite convinced of our model's ability to classify emojis. We don't want to send a faulty model to the Unicode Consortium, the consequences would be disastrous! üôÄ In the `test` data directory is the rainbow flag. Let's put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j39_aJOgcaTG"
   },
   "outputs": [],
   "source": [
    "x_img = Image.open(\"emojis/test/pride.png\")\n",
    "x_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxkuFCJcNyF-"
   },
   "source": [
    "The rainbow flag is colorful üåà but we need it to be in grayscale. Remember: always apply the same transformations to the prediction data as the training data!\n",
    "\n",
    "We use [pillow](https://pillow.readthedocs.io/en/stable/) to convert the image to grayscale, then reshape it, and create a `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAGXj7GMVP4_"
   },
   "outputs": [],
   "source": [
    "x_img_gray = x_img.convert('L')\n",
    "x_arr = np.array(x_img_gray).reshape(1, 1, 64, 64)\n",
    "x_predict = torch.tensor(x_arr, dtype=torch.float32)\n",
    "\n",
    "x_predict.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a05lxgx6OJDL"
   },
   "source": [
    "The batch size, channel, height, and width dimensions are present, so let's send the feature `Tensor` to the `device`, since our `net` parameters are already located there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPNdp8jgKmef"
   },
   "outputs": [],
   "source": [
    "x_predict = x_predict.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4OIEfodPETA"
   },
   "source": [
    "We can now feed the features to our CNN. We use the `torch.no_grad()` context since we don't plan to call `.backward()` on the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NoBSq1FWPTLo"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  logits = net(x_predict)\n",
    "  print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rgtas1i6PZUL"
   },
   "source": [
    "Recall that we used the `CrossEntropyLoss()` which incorporates the softmax operation. This means that our model returns _logits_, and we have to apply the softmax ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rl0NtPQ7Piif"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  probas = torch.softmax(logits, dim=1)\n",
    "  print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwrkp2_IPnCq"
   },
   "source": [
    "This is a _probability vector_ which tells us the likelihood of this emoji belonging to each class: `0`, `1`, or `2`. Our CNN seems _very_ confident here, but often these probabilities are more even. They always add up to 1 though! We can find the most likely class with an argmax operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qv7p8i-8PE12"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  _, y_predict = torch.max(probas, 1)\n",
    "  print(y_predict.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbqSJ8q6O3ei"
   },
   "source": [
    "Remember that we defined class integers as:\n",
    "- face: 0\n",
    "- flag: 1\n",
    "- animals: 2\n",
    "\n",
    "Our model identified the emoji as a flag! üè≥Ô∏è‚Äçüåà We fed in an image that our CNN had _never seen before_ , and it correctly classified it. The power of machine learning... üßô‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsCTa5fDcLAY"
   },
   "source": [
    "Let's test our model even further, with emojis that the world has never seen. üòÆ Create your own emoji submission to the Unicode Consortium, and then check if our CNN correctly classifies it.\n",
    "\n",
    "üí™üí™ Make a face emoji [here](https://emoji-maker.com/designer), save it in your local directory / upload it to your Google colab environment (by selecting \"upload\" on the menu on the left hand side). Then check our trained CNN's class prediction.\n",
    "- you might need to [crop](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=crop#PIL.Image.Image.crop) the image to make it square\n",
    "- preprocess the image in the same way as we did during training: our CNN expects a 64x64 grayscale image\n",
    "- don't forget about batch size and channel dimensions\n",
    "- use the `torch.no_grad()` context\n",
    "- feel free to message me your creation / classification, I'm curious about the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-c7DJs_YVV-o"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Df86o52CZNPm"
   },
   "source": [
    "## 5. Summary\n",
    "\n",
    "\n",
    "Today, we learned about **Convolutional Neural Networks**. We defined **computer vision** as the field specializing in image data analysis. We pointed out a few standout quirks of image data: **high dimensionality**, **translational invariance**, and **local redundancy**. We discovered **convolutional neural layers**, and showed how they leverage translational invariance to **share model parameters** across sliding input windows. This helps with the **computational complexity** of modeling so many feature dimensions. We then highlighted how **pooling layers** further improve network efficiency in the deeper layers by **downsampling** activation tensors, and discarding **locally redundant** information. When stacked with **fully connected layers**, these form **convolutional neural networks**. We listed some CNN **design conventions** , but noted that these architecture choices are **complex**. Finally, we helped the Unicode Consortium by implementing and training our own CNN **image classifier** in pytorch, to identify `face`, `flag`, and `animal` emojis.\n",
    "\n",
    "# Resources\n",
    "\n",
    "##¬†Core Resources\n",
    "\n",
    "- [cs231n](https://cs231n.github.io/convolutional-networks/)  \n",
    "Classic course on convolutional neural networks\n",
    "- [introduction to CNNs](https://victorzhou.com/blog/intro-to-cnns-part-1/)  \n",
    "Excellent visual blogpost which implements a CNN from scratch with NumPy\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Deep learning school - DL for CV](https://youtu.be/u6aEYuemt0M)  \n",
    "Karpathy lecture explaining CNN architectures\n",
    "- [Convolution explained](https://youtu.be/N-zd-T17uiE)  \n",
    "Youtube video detailing the maths of convolution\n",
    "- [Translational invariance in CNNs](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo)  \n",
    "Stackexchange thread outlining how convolutions relate to translational invariance & equivariance\n",
    "- [how to choose MNIST CNN architecture](https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist)  \n",
    "Kaggle kernel trying different CNN architectures on the MNIST dataset\n",
    "- [PyTorch implementations of CNNs](https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/tree/master/pytorch_ipynb/cnn/)  \n",
    "Pytorch CNNs from the deeplearning models repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmbVhrS1UtbG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "computer_vision_pt.1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
