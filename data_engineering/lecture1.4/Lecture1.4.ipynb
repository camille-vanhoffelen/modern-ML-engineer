{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1.4: Data Pipelines\n",
    "\n",
    "This lecture, we are going to implement a small data pipeline in python. It will take raw text files from a \"data warehouse\", extract, transform, then load them back into a different dataset. Then, we will automate this pipeline by scheduling it with cron.\n",
    "\n",
    "**Learning goals:**\n",
    "- implement a simple data pipeline\n",
    "- write python functions\n",
    "- write a python script\n",
    "- learn python File I/O\n",
    "- schedule a cron task with crontab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Reading Files\n",
    "\n",
    "We are in the middle of a global pandemic, and there's a lot of information traded online, especially with so many of us are confined in our homes. Twitter is a common place where data scientists gauge public opinion or group behaviour. In our data warehouse, we have ~ 100 tweets containing the `#corona` hashtag. We want to get some insights into the epidemic by analysing the text data. \n",
    "\n",
    "For this, we'll have to open files in python. But first, let's investigate what we have in our data warehouse. \n",
    "\n",
    "â„¹ï¸ You can run bash commands in a jupyter notebook by preceding them with a a bang (`!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data_warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One directory, `tweets`. Let's see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data_warehouse/tweets | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our tweets! Now we want to read one of these tweets. We could use `cat` or a text editor, but let's try loading the data directly in python. \n",
    "\n",
    "ðŸ’ª With the help of this [documentation](https://docs.python.org/3/tutorial/inputoutput.html), open the contents of `0a63eb60b0ab4222ab6bb63bac648d8f.txt` in python! Display the tweet in clear text in the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Congrats! You should have used the `with` keyword. If not, please try again. Can you explain why the  `with` keyword is considered good practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’ª Now let's make a function to open a text file for a given path, it'll be much easier to use. Copy your working code from the cell above to complete this function. Your function should return a `string` of the text inside the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out. If the following code cell doesn't return the tweet content, go back and try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file('data_warehouse/tweets/c03af4427b6e45728a483d7a75cc8651.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! It looks like some of the `#corona` tweets are about beer ðŸº, not the virus ðŸ˜·... This might become a problem if we try to analyse the data. But fear not, we can _clean_ the data before we use it for anything. In this case, we want to filter out all tweets that are about beer rather than viruses. There are complex ways of doing this, but we'll stick to a simple solution: if a tweet contains the word `beer`, we'll assume it's talking about the corona brand rather than the COVID-19 virus. \n",
    "\n",
    "Our aim here, is to build a **data pipeline** that will extract our tweets from the data warehouse, filter them, and load them again in a new `covid_tweets` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Manipulating Text\n",
    "\n",
    "In the previous section, we've extracted tweets from the data warehouse. Now we would like to filter out the ones about beer.\n",
    "\n",
    "ðŸ’ª Write a function that takes in text, and returns `True` if the text contains the word `beer`, `False` if it doesn't. Since we're catching the meaning of beer here, keep in mind that it should work for all letter cases!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_beer(text):\n",
    "    # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to test this function. For this, we can use a **unit test**. Unit tests are functions whose sole purpose is to test the behaviours of other functions and classes. They are crucial in software development because they allow to catch bugs early. We'll cover unit testing in more detail later in the course, but you should remember that one of the main advantages of writing proper data pipelines instead of manually moving things around... is that we can write tests!\n",
    "\n",
    "ðŸ’ª The cell below defines and executes a unit test. Simply run the cell to test your `.contains_beer()` function. If the test fails, try fixing your function above and try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_contains_beer():\n",
    "    assert not contains_beer('This sentence is about viruses')\n",
    "    assert contains_beer('This sentence is about beer')\n",
    "    assert contains_beer('ThIs SeNtEnCe Is AbOuT BeeR!')\n",
    "    print('Success! ðŸŽ‰')\n",
    "    \n",
    "test_contains_beer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Can you explain what the [`assert`](https://stackoverflow.com/questions/5142418/what-is-the-use-of-assert-in-python) keyword does in this unit test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Iterating Through Files\n",
    "\n",
    "We can now read a tweet, and analyse if it speaks of viruses or beers. But our data pipeline must run on the entire dataset, so we have to iterate through all the files. We can use [glob](https://docs.python.org/3/library/glob.html), which allows us to search and list file paths.\n",
    "\n",
    "The cell below lists the tweet file paths, i.e all `.txt` files located under the directory `tweets`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# get the list of tweet paths\n",
    "tweet_paths = glob.glob('data_warehouse/tweets/*.txt')\n",
    "# print the first ten\n",
    "tweet_paths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we can see the list of tweet files, just like the `ls` unix command, but in python. Now that we can list,  read, and filter files, we are close to finishing our data pipeline! Let's figure out how many of our tweets are about beer rather than viruses.\n",
    "\n",
    "ðŸ’ªðŸ’ª In the cell below, write a for loop which iterates through the `tweet_paths` and for each tweet path:\n",
    "- read the file\n",
    "- print the text if it contains beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  You should have found seven. Now that we know which tweets are not about COVID-19, we could now manually remove each one of these text files. Can you think of reasons why that's a bad idea in practice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Writing Files\n",
    "\n",
    "Recall that a data pipeline extracts, transforms, and loads data. It is considered bad practice to modify datasets in place (ðŸ§  Can you tell why that's a bad idea?). This means data pipelines can _filter_ data by simply not loading the undesirable data in the final stage. i.e to clean our tweets, we are _only_ going to copy tweets about viruses to a new dataset. To do this, we need to know how to write files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’ª With the help of this [documentation](https://docs.python.org/3/tutorial/inputoutput.html), code a function to write content to files. The function should take two arguments: first, the path of the file to create, then the content we wish to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the function worked by using the [`cat`](https://en.wikipedia.org/wiki/Cat_(Unix)) unix command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('current_mood.txt', 'Learning about data pipelines and beer! ðŸ»\\n')\n",
    "!cat current_mood.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the string isn't shown in the cell output, modify your `write_file` function and try again!\n",
    "\n",
    "We've learned that versioning our data pipelines is important. It's also useful to add some details about what the data represents. We want to add a simple `README.txt` file to the new dataset with this information. \n",
    "\n",
    "ðŸ§  Why is versioning our data pipelines important?\n",
    "\n",
    "ðŸ’ª Write a function which can write a `README.txt` for a given output path, and a data pipeline version. Pro-tip: you can reuse the `write_file` function you wrote above. Be as verbose as you wish with the content! One example might be:\n",
    "> #corona tweets about covid19.  \n",
    "> Beer tweets filtered with filter_beer v1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_readme(output_dir, version):\n",
    "    # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function works, the unit test below should pass. If not, please try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_write_readme():\n",
    "    write_readme('.', '1.0')\n",
    "    content = !cat README.txt\n",
    "    !rm README.txt\n",
    "    assert content\n",
    "    print('Success! ðŸŽ‰')\n",
    "    \n",
    "test_write_readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ ðŸ§  Observe that the `content` variable is _not_ a boolean. Can you explain what the `assert` does here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to write our data pipeline! \n",
    "\n",
    "ðŸ’ªðŸ’ª Write a function called `filter_beer` which takes in an input directory path (the input dataset folder), and an output directory path (the output dataset folder). The function should iterate through the files found in the `input_dir`, read their contents, check if they are about beer, and only write those about viruses back into the `output_dir`. Finally, the function should write a `README.txt` file with extra versioning and dataset information.\n",
    "\n",
    "This may seem like a lot, but you have already written a lot of this in your functions above! Make sure to re-use them here. This includes `.read_file()`, `.contains_beer()`, `.write_file()`, and `.write_readme()`. You'll also need to use the `glob` library to iterate through the files.\n",
    "\n",
    "I've also included the `file_name` function, which returns the file name for a given path. e.g `some/path/to/a/file.txt` returns `file.txt`. This should come handy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def file_name(path):\n",
    "    return os.path.basename(path)\n",
    "\n",
    "def filter_beer(input_dir, output_dir):\n",
    "    # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit test below will check if your function correctly filtered the beer tweets out of the new `covid_tweets` dataset. If it fails, please try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_filter_beer():\n",
    "    # make new dataset directory\n",
    "    !mkdir -p data_warehouse/covid_tweets\n",
    "    # clean it out just in case\n",
    "    !rm -f data_warehouse/covid_tweets/*.txt\n",
    "    # run the filter_beer data pipeline\n",
    "    filter_beer('data_warehouse/tweets', 'data_warehouse/covid_tweets')\n",
    "    # list and count the new files\n",
    "    result = !ls data_warehouse/covid_tweets | wc -l\n",
    "    covid_tweets_count = int(result[0])\n",
    "    # check that the count is correct\n",
    "    assert covid_tweets_count == 101\n",
    "    \n",
    "test_filter_beer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data pipeline to clean our tweets. It's still fairly inconvenient to have to run this function every time new tweets come in (and there are a _lot_ of tweets posted every second!). We want to automate the data pipeline using [`cron`](https://en.wikipedia.org/wiki/Cron). For this, we first have to take our function, and turn it into a python script. \n",
    "\n",
    "ðŸ’ª Copy paste your `filter_beer` method in the `filter_beer.py` file in this directory. I've already added other useful methods inside of it, so you shouldn't have to copy any of your other functions. For this, direclty use your shell or file explorer. You can use any text editor, [sublime](https://www.sublimetext.com/) is a good  choice!  \n",
    "Pro-tip: if you are looking for the file's path, remember you can run `!pwd` in a code cell of this notebook.\n",
    "\n",
    "As you may have noticed from the `main()` method, this script will run the data pipeline from a directory called `input` into a directory called `output`. There is one tweet in `input` right now, let's see if the script works. The following cell should return the file name of one tweet. If not, try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how to run python code files\n",
    "!python filter_beer.py\n",
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can run our data pipeline as a python script, we can add to our [`crontab`](https://www.adminschoice.com/crontab-quick-reference). The cell below pulls your python binary and directory information to produce the appropriate `crontab` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = !which python\n",
    "where = !pwd\n",
    "line = ''.join(['* * * * * ', 'cd ', where[0], ' && ', python_path[0] , ' ' , where[0] , '/filter_beer.py'])\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `* * * * *` starting the line indicates `cron` that we wish to schedule this command to run _every minute_. Please note that this is not a realistic data pipeline scenario, and that to get real-time processing of data one should consider streaming solutions. But `cron` is a simple and effective solution if you want to automate your data pipelines and need them to run every day, week, ... Nethertheless, for the purpose of illustration, we'll keep our schedule fast so that we can (almost) immediately see results. \n",
    "\n",
    "ðŸ’ª Add the output of the cell above to your `crontab` file. To do this, run `crontab -e` in your shell and paste the line with your chosen editor. If you are getting some errors on the ubuntu shell for windows, please try [this](https://stackoverflow.com/questions/41281112/crontab-not-working-with-bash-on-ubuntu-on-windows).\n",
    "\n",
    "Now let's check to see if our scheduled data pipeline works! Run the cell below to copy a tweet into the `input` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp data_warehouse/tweets/65fc22b30e2e46fb85bba02d81f914bd.txt input\n",
    "!ls input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input` directory should now contain 2 files. Now your automated data pipeline should be in the process of extracting, transforming, and loading your data! **Please wait a couple minutes** before running the cell below (or your croned job won't have had the time to be scheduled yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return _the same_ files as for the `input` directory above. If not, please update your `crontab` and try again!\n",
    "\n",
    "We have shown that our data pipeline moved files from one dataset to another, but we haven't proved that it can successfully filter out tweets about beers.\n",
    "\n",
    "ðŸ’ª Using `cp`, copy two files from the `tweets` dataset in the data warehouse to the `input` directory in the cell below. The first tweet should be `571d832e07bc451caa1bc9ecd98ef4c5.txt` (about viruses), the second `6601dc27f19347aab553582228a79f1c.txt` (about beer). Then check that they were correctly copied using `ls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have made sure that the new files were added to the `input` directory, **please wait a couple of minutes**, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  What happened? Is this what you expected? Please summarise every step involved in the process.\n",
    "\n",
    "ðŸ§ ðŸ§  Can you list ways in which an orchestration framework (e.g: `airflow`, `luigi`) would improve our `cron` scheduling?\n",
    "\n",
    "ðŸ’ª Feel free to add more files to `input` and check what is filtered from `output`. You can even write your own tweets! \n",
    "\n",
    "Last but not least, let's remove our data pipeline scheduling so that your machine doesn't try to filter beer tweets every minute for the rest of time ðŸ˜….\n",
    "\n",
    "**Warning**: this will delete your crontab entries. If you are a regular crontab user, please remove the appropriate line using `crontab -e` instead. If you have never used crontab before, no worries, please continue! \n",
    "\n",
    "ðŸ’ª Run the cell below to clear out your crontab and remove the data pipeline automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!crontab -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Summary\n",
    "\n",
    "Today, we have learned about **data pipelines**, and how they are a key part of modern data architectures. We have seen how they can be chained into **complex workflows**, often **orchestrated** by frameworks. We have seen how they can be improved by **automation**, **testing**, and **versioning**. We also heard a couple of tips on how to avoid writing complicated obscure code, and instead aiming for **straightforward, reusable tools**.\n",
    "\n",
    "In this notebook we also implemented a simple data pipeline to filter out tweets about beer in our `#corona` twitter dataset. To do this, we used python file i/o functions to iterate through, read, and write files. Finally, we scheduled the python script in `cron`, and saw our automated data pipeline in action! ðŸ¤–\n",
    "\n",
    "# Resources\n",
    "## Core Resources\n",
    "- [**Slides**](https://docs.google.com/presentation/d/1KLTh7NLoTnKFRZsIIthi8YEH_l4U1NEZzwmctqWrUeo/edit?usp=sharing)\n",
    "- [What is a data pipeline](https://www.alooma.com/blog/what-is-a-data-pipeline)\n",
    "- [python i/o documentation](https://docs.python.org/3/tutorial/inputoutput.html) \n",
    "- [glob docs](https://docs.python.org/3/library/glob.html)\n",
    "- [crontab for WSL](https://stackoverflow.com/questions/41281112/crontab-not-working-with-bash-on-ubuntu-on-windows)\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [crontab guru](https://crontab.guru/)  \n",
    "Simple website to test your crontab configurations\n",
    "- [A beginner's guide to data engineering](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7)  \n",
    "In depth 3 part post on data pipelines at airbnb\n",
    "- [Orchestrating batch processing pipelines with cron and make](https://snowplowanalytics.com/blog/2015/10/13/orchestrating-batch-processing-pipelines-with-cron-and-make/)  \n",
    "Lightweight DIY alternative to data pipeline management tools.\n",
    "- [Airflow a cron replacement for data pipelines](https://medium.com/@rbahaguejr/airflow-a-beautiful-cron-alternative-or-replacement-for-data-pipelines-b6fb6d0cddef)  \n",
    "Simple working example of data pipeline management with Airflow.\n",
    "- [Luigi vs Airflow](https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7)  \n",
    "Summary with code of the differences between two popular data pipeline management tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
